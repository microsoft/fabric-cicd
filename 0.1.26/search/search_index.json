{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>fabric-cicd is a Python library designed for use with Microsoft Fabric workspaces. This library supports code-first Continuous Integration / Continuous Deployment (CI/CD) automations to seamlessly integrate Source Controlled workspaces into a deployment framework. The goal is to assist CI/CD developers who prefer not to interact directly with the Microsoft Fabric APIs.</p>"},{"location":"#base-expectations","title":"Base Expectations","text":"<ul> <li>Full deployment every time, without considering commit diffs</li> <li>Deploys into the tenant of the executing identity</li> <li>Only supports items that have Source Control, and Public Create/Update APIs</li> </ul>"},{"location":"#supported-item-types","title":"Supported Item Types","text":"<ul> <li>DataPipeline</li> <li>Environment</li> <li>Notebook</li> <li>Report</li> <li>SemanticModel</li> <li>Lakehouse</li> <li>MirroredDatabase</li> <li>VariableLibrary</li> <li>CopyJob</li> <li>Eventhouse</li> <li>KQLDatabase</li> <li>KQLQueryset</li> <li>Reflex</li> <li>Eventstream</li> <li>Warehouse</li> <li>SQLDatabase</li> <li>KQLDashboard</li> <li>Dataflow</li> <li>GraphQLApi</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Requirements: Python 3.9 to 3.12</p> <p>To install fabric-cicd, run:</p> <pre><code>pip install fabric-cicd\n</code></pre>"},{"location":"#basic-example","title":"Basic Example","text":"<pre><code>from fabric_cicd import FabricWorkspace, publish_all_items, unpublish_all_orphan_items\n\n# Initialize the FabricWorkspace object with the required parameters\ntarget_workspace = FabricWorkspace(\n    workspace_id = \"your-workspace-id\",\n    environment = \"your-target-environment\",\n    repository_directory = \"your-repository-directory\",\n    item_type_in_scope = [\"Notebook\", \"DataPipeline\", \"Environment\"],\n)\n\n# Publish all items defined in item_type_in_scope\npublish_all_items(target_workspace)\n\n# Unpublish all items defined in item_type_in_scope not found in repository\nunpublish_all_orphan_items(target_workspace)\n</code></pre> <p>Note: The <code>environment</code> parameter is required for parameter replacement to work properly. It must match one of the environment keys defined in your <code>parameter.yml</code> file (e.g., \"PPE\", \"PROD\", \"DEV\"). If you don't need parameter replacement, you can omit this parameter.</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#support","title":"Support","text":"<p>This project uses GitHub Issues to track bugs, feature requests, and questions. Please search the existing issues before filing new issues to avoid duplicates. For new issues, file your bug, feature request, or question as a new Issue.</p>"},{"location":"about/#microsoft-support-policy","title":"Microsoft Support Policy","text":"<p>Support for this PROJECT or PRODUCT is limited to the resources listed above.</p>"},{"location":"about/#security","title":"Security","text":"<p>Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include Microsoft, Azure, DotNet, AspNet and Xamarin.</p> <p>If you believe you have found a security vulnerability in any Microsoft-owned repository that meets Microsoft's definition of a security vulnerability, please report it to us as described below.</p>"},{"location":"about/#reporting-security-issues","title":"Reporting Security Issues","text":"<p>Please do not report security vulnerabilities through public GitHub issues.</p> <p>Instead, please report them to the Microsoft Security Response Center (MSRC) at https://msrc.microsoft.com/create-report.</p> <p>If you prefer to submit without logging in, send email to secure@microsoft.com. If possible, encrypt your message with our PGP key; please download it from the Microsoft Security Response Center PGP Key page.</p> <p>You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at microsoft.com/msrc.</p> <p>Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:</p> <ul> <li>Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)</li> <li>Full paths of source file(s) related to the manifestation of the issue</li> <li>The location of the affected source code (tag/branch/commit or direct URL)</li> <li>Any special configuration required to reproduce the issue</li> <li>Step-by-step instructions to reproduce the issue</li> <li>Proof-of-concept or exploit code (if possible)</li> <li>Impact of the issue, including how an attacker might exploit the issue</li> </ul> <p>This information will help us triage your report more quickly.</p> <p>If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our Microsoft Bug Bounty Program page for more details about our active programs.</p>"},{"location":"about/#preferred-languages","title":"Preferred Languages","text":"<p>We prefer all communications to be in English.</p>"},{"location":"about/#policy","title":"Policy","text":"<p>Microsoft follows the principle of Coordinated Vulnerability Disclosure.</p>"},{"location":"about/#license","title":"License","text":"<pre><code>MIT License\n\nCopyright (c) Microsoft Corporation.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":"<p>The following contains all major, minor, and patch version release notes.</p> <ul> <li>\ud83d\udca5 Breaking change!</li> <li>\u2728 New Functionality</li> <li>\ud83d\udd27 Bug Fix</li> <li>\ud83d\udcdd Documentation Update</li> <li>\u26a1 Internal Optimization</li> </ul>"},{"location":"changelog/#version-0126","title":"Version 0.1.26","text":"<p>Release Date: 2025-09-05</p> <ul> <li>\ud83d\udca5 Deprecate Base API URL kwarg in Fabric Workspace (#529)</li> <li>\u2728 Support Schedules parameterization (#508)</li> <li>\u2728 Support YAML configuration file-based deployment (#470)</li> <li>\ud83d\udcdd Add dynamically generated Python version requirements to documentation (#520)</li> <li>\u26a1 Enhance pytest output to limit console verbosity (#514)</li> <li>\ud83d\udd27 Fix Report item schema handling (#518)</li> <li>\ud83d\udd27 Fix deployment order to publish Mirrored Database before Lakehouse (#482)</li> </ul>"},{"location":"changelog/#version-0125","title":"Version 0.1.25","text":"<p>Release Date: 2025-08-19</p> <ul> <li>\ud83d\udca5 Modify the default for item_types_in_scope and add thorough validation (#464)</li> <li>\u2728 Add new experimental feature flag to enable selective deployment (#384)</li> <li>\u2728 Support \"ALL\" environment concept in parameterization (#320)</li> <li>\ud83d\udcdd Enhance Overview section in Parameterization docs (#495)</li> <li>\u26a1 Eliminate ACCEPTED_ITEM_TYPES_NON_UPN constant and unify with ACCEPTED_ITEM_TYPES (#477)</li> <li>\u26a1 Add comprehensive GitHub Copilot instructions for effective codebase development (#468)</li> <li>\ud83d\udd27 Add feature flags and warnings for Warehouse, SQL Database, and Eventhouse unpublish operations (#483)</li> <li>\ud83d\udd27 Fix code formatting inconsistencies in fabric_workspace unit test (#474)</li> <li>\ud83d\udd27 Fix KeyError when deploying Reports with Semantic Model dependencies in Report-only scope case (#278)</li> </ul>"},{"location":"changelog/#version-0124","title":"Version 0.1.24","text":"<p>Release Date: 2025-08-04</p> <ul> <li>\ud83d\udca5 Require parameterization for Dataflow and Semantic Model references in Data Pipeline activities</li> <li>\ud83d\udca5 Require specific parameterization for deploying a Dataflow that depends on another in the same workspace (see Parameterization docs)</li> <li>\ud83d\udd27 Fix Dataflow/Data Pipeline deployment failures caused by workspace permissions (#419)</li> <li>\ud83d\udd27 Prevent duplicate logical ID issue in Report and Semantic Model deployment (#405)</li> <li>\ud83d\udd27 Fix deployment of items without assigned capacity (#402)</li> <li>\ud83d\udcdd Improve Parameterization documentation (#415)</li> <li>\u26a1 Support for Eventhouse query URI parameterization (#414)</li> <li>\u26a1 Support for Warehouse SQL endpoint parameterization (#392)</li> </ul>"},{"location":"changelog/#version-0123","title":"Version 0.1.23","text":"<p>Release Date: 2025-07-08</p> <ul> <li>\u2728 New functionalities for GitHub Copilot Agent and PR-to-Issue linking</li> <li>\ud83d\udd27 Fix issue with lakehouse shortcuts publishing ([#379] (https://github.com/microsoft/fabric-cicd/issues/379))</li> <li>\ud83d\udd27 Add validation for empty logical IDs to prevent deployment corruption (#86)</li> <li>\ud83d\udd27 Fix SQL provision print statement (#329)</li> <li>\ud83d\udd27 Rename the error code for reserved item name per updated Microsoft Fabric API (#388)</li> <li>\ud83d\udd27 Fix lakehouse exclude_regex to exclude shortcut publishing (#385)</li> <li>\ud83d\udd27 Remove max retry limit to handle large deployments ([#299] (https://github.com/microsoft/fabric-cicd/issues/299))</li> <li>\ud83d\udcdd Fix formatting and examples in the How to and Examples pages</li> </ul>"},{"location":"changelog/#version-0122","title":"Version 0.1.22","text":"<p>Release Date: 2025-06-25</p> <ul> <li>\u2728 Onboard API for GraphQL item type (#287)</li> <li>\ud83d\udd27 Fix Fabric API call error during dataflow publish (#352)</li> <li>\u26a1 Expanded test coverage to handle folder edge cases (#358)</li> </ul>"},{"location":"changelog/#version-0121","title":"Version 0.1.21","text":"<p>Release Date: 2025-06-18</p> <ul> <li>\ud83d\udd27 Fix bug with workspace ID replacement in JSON files for pipeline deployments (#345)</li> <li>\u26a1 Increased max retry for Warehouses and Dataflows</li> </ul>"},{"location":"changelog/#version-0120","title":"Version 0.1.20","text":"<p>Release Date: 2025-06-12</p> <ul> <li>\u2728 Onboard KQL Dashboard item type (#329)</li> <li>\u2728 Onboard Dataflow Gen2 item type (#111)</li> <li>\u2728 Parameterization support for find_value regex and replace_value variables (#326)</li> <li>\ud83d\udd27 Fix bug with deploying environment libraries with special chars (#336)</li> <li>\u26a1 Improved test coverage for subfolder creation/modification (#211)</li> </ul>"},{"location":"changelog/#version-0119","title":"Version 0.1.19","text":"<p>Release Date: 2025-05-21</p> <ul> <li>\u2728 Onboard SQL Database item type (shell-only deployment) (#301)</li> <li>\u2728 Onboard Warehouse item type (shell-only deployment) (#204)</li> <li>\ud83d\udd27 Fix bug with unpublish workspace folders (#273)</li> </ul>"},{"location":"changelog/#version-0118","title":"Version 0.1.18","text":"<p>Release Date: 2025-05-14</p> <ul> <li>\ud83d\udd27 Fix bug with check environment publish state (#295)</li> </ul>"},{"location":"changelog/#version-0117","title":"Version 0.1.17","text":"<p>Release Date: 2025-05-13</p> <ul> <li>\ud83d\udca5 Deprecate old parameter file structure (#283)</li> <li>\u2728 Onboard CopyJob item type (#122)</li> <li>\u2728 Onboard Eventstream item type (#170)</li> <li>\u2728 Onboard Eventhouse/KQL Database item type (#169)</li> <li>\u2728 Onboard Data Activator item type (#291)</li> <li>\u2728 Onboard KQL Queryset item type (#292)</li> <li>\ud83d\udd27 Fix post publish operations for skipped items (#277)</li> <li>\u26a1 New function <code>key_value_replace</code> for key-based replacement operations in JSON and YAML</li> <li>\ud83d\udcdd Add publish regex example to demonstrate how to use the <code>publish_all_items</code> with regex for excluding item names</li> </ul>"},{"location":"changelog/#version-0116","title":"Version 0.1.16","text":"<p>Release Date: 2025-04-25</p> <ul> <li>\ud83d\udd27 Fix bug with folder deployment to root (#255)</li> <li>\u26a1 Add Workspace Name in FabricWorkspaceObject (#200)</li> <li>\u26a1 New function to check SQL endpoint provision status (#226)</li> <li>\ud83d\udcdd Updated Authentication docs + menu sort order</li> </ul>"},{"location":"changelog/#version-0115","title":"Version 0.1.15","text":"<p>Release Date: 2025-04-21</p> <ul> <li>\ud83d\udd27 Fix folders moving with every publish (#236)</li> <li>\u26a1 Introduce parallel deployments to reduce publish times (#237)</li> <li>\u26a1 Improvements to check version logic</li> <li>\ud83d\udcdd Updated Examples section in docs</li> </ul>"},{"location":"changelog/#version-0114","title":"Version 0.1.14","text":"<p>Release Date: 2025-04-09</p> <ul> <li>\u2728 Optimized &amp; beautified terminal output</li> <li>\u2728 Added changelog to output of old version check</li> <li>\ud83d\udd27 Fix workspace folder deployments in root folder (#221)</li> <li>\ud83d\udd27 Fix unpublish of workspace folders without publish (#222)</li> <li>\u26a1 Removed Colorama and Colorlog Dependency</li> </ul>"},{"location":"changelog/#version-0113","title":"Version 0.1.13","text":"<p>Release Date: 2025-04-07</p> <ul> <li>\u2728 Onboard Workspace Folders (#81)</li> <li>\u2728 Onboard Variable Library item type (#206)</li> <li>\u2728 Added support for Lakehouse Shortcuts</li> <li>\u2728 New <code>enable_environment_variable_replacement</code> feature flag (#160)</li> <li>\u26a1 User-agent now available in API headers (#207)</li> <li>\u26a1 Fixed error log typo in fabric_endpoint</li> <li>\ud83d\udd27 Fix break with invalid optional parameters (#192)</li> <li>\ud83d\udd27 Fix bug where all workspace ids were not being replaced by parameterization (#186)</li> </ul>"},{"location":"changelog/#version-0112","title":"Version 0.1.12","text":"<p>Release Date: 2025-03-27</p> <ul> <li>\ud83d\udd27 Fix constant overwrite failures (#190)</li> <li>\ud83d\udd27 Fix bug where all workspace ids were not being replaced (#186)</li> <li>\ud83d\udd27 Fix type hints for older versions of Python (#156)</li> <li>\ud83d\udd27 Fix accepted item types constant in pre-build</li> </ul>"},{"location":"changelog/#version-0111","title":"Version 0.1.11","text":"<p>Release Date: 2025-03-25</p> <ul> <li>\ud83d\udca5 Parameterization refactor introducing a new parameter file structure and parameter file validation functionality (#113)</li> <li>\ud83d\udcdd Update to parameterization docs</li> <li>\u2728 Support regex for publish exclusion (#121)</li> <li>\u2728 Override max retries via constants (#146)</li> </ul>"},{"location":"changelog/#version-0110","title":"Version 0.1.10","text":"<p>Release Date: 2025-03-19</p> <ul> <li>\u2728 DataPipeline SPN Support (#133)</li> <li>\ud83d\udd27 Workspace ID replacement in data pipelines (#164)</li> <li>\ud83d\udcdd Sample for passing in arguments from Azure DevOps Pipelines</li> </ul>"},{"location":"changelog/#version-019","title":"Version 0.1.9","text":"<p>Release Date: 2025-03-11</p> <ul> <li>\u2728 Support for Mirrored Database item type (#145)</li> <li>\u26a1 Increase reserved name wait time (#135)</li> </ul>"},{"location":"changelog/#version-018","title":"Version 0.1.8","text":"<p>Release Date: 2025-03-04</p> <ul> <li>\ud83d\udd27 Handle null byPath object in report definition file (#143)</li> <li>\ud83d\udd27 Support relative directories (#136) (#132)</li> <li>\ud83d\udd27 Increase special character support (#134)</li> <li>\u26a1 Changelog now available with version check (#127)</li> </ul>"},{"location":"changelog/#version-017","title":"Version 0.1.7","text":"<p>Release Date: 2025-02-26</p> <ul> <li>\ud83d\udd27 Fix special character support in files (#129)</li> </ul>"},{"location":"changelog/#version-016","title":"Version 0.1.6","text":"<p>Release Date: 2025-02-24</p> <ul> <li>\u2728 Onboard Lakehouse item type (#116)</li> <li>\ud83d\udcdd Update example docs (#25)</li> <li>\ud83d\udcdd Update find_replace docs (#110)</li> <li>\u26a1 Standardized docstrings to Google format</li> <li>\u26a1 Onboard file objects (#46)</li> <li>\u26a1 Leverage UpdateDefinition Flag (#28)</li> <li>\u26a1 Convert repo and workspace dictionaries (#45)</li> </ul>"},{"location":"changelog/#version-015","title":"Version 0.1.5","text":"<p>Release Date: 2025-02-18</p> <ul> <li>\ud83d\udd27 Fix Environment Failure without Public Library (#103)</li> <li>\u26a1 Introduces pytest check for PRs (#100)</li> </ul>"},{"location":"changelog/#version-014","title":"Version 0.1.4","text":"<p>Release Date: 2025-02-12</p> <ul> <li>\u2728 Support Feature Flagging (#96)</li> <li>\ud83d\udd27 Fix Image support in report deployment (#88)</li> <li>\ud83d\udd27 Fix Broken README link (#92)</li> <li>\u26a1 Workspace ID replacement improved</li> <li>\u26a1 Increased error handling in activate script</li> <li>\u26a1 Onboard pytest and coverage</li> <li>\u26a1 Improvements to nested dictionaries (#37)</li> <li>\u26a1 Support Python Installed From Windows Store (#87)</li> </ul>"},{"location":"changelog/#version-013","title":"Version 0.1.3","text":"<p>Release Date: 2025-01-29</p> <ul> <li>\u2728 Add PyPI check version to encourage version bumps (#75)</li> <li>\ud83d\udd27 Fix Semantic model initial publish results in None Url error (#61)</li> <li>\ud83d\udd27 Fix Integer parsed as float failing in handle_retry for &lt;3.12 python (#63)</li> <li>\ud83d\udd27 Fix Default item types fail to unpublish (#76)</li> <li>\ud83d\udd27 Fix Items in subfolders are skipped (#77)</li> <li>\ud83d\udcdd Update documentation &amp; examples</li> </ul>"},{"location":"changelog/#version-012","title":"Version 0.1.2","text":"<p>Release Date: 2025-01-27</p> <ul> <li>\u2728 Introduces max retry and backoff for long running / throttled calls (#27)</li> <li>\ud83d\udd27 Fix Environment publish uses arbitrary wait time (#50)</li> <li>\ud83d\udd27 Fix Environment publish doesn't wait for success (#56)</li> <li>\ud83d\udd27 Fix Long running operation steps out early for notebook publish (#58)</li> </ul>"},{"location":"changelog/#version-011","title":"Version 0.1.1","text":"<p>Release Date: 2025-01-23</p> <ul> <li>\ud83d\udd27 Fix Environment stuck in publish (#51)</li> </ul>"},{"location":"changelog/#version-010","title":"Version 0.1.0","text":"<p>Release Date: 2025-01-23</p> <ul> <li>\u2728 Initial public preview release</li> <li>\u2728 Supports Notebook, Pipeline, Semantic Model, Report, and Environment deployments</li> <li>\u2728 Supports User and System Identity authentication</li> <li>\u2728 Released to PyPi</li> <li>\u2728 Onboarded to Github Pages</li> </ul>"},{"location":"code_reference/","title":"Code Reference","text":"<p>Provides tools for managing and publishing items in a Fabric workspace.</p> <p>Classes:</p> Name Description <code>FabricWorkspace</code> <p>A class to manage and publish workspace items to the Fabric API.</p> <p>Functions:</p> Name Description <code>append_feature_flag</code> <p>Append a feature flag to the global feature_flag set.</p> <code>change_log_level</code> <p>Sets the log level for all loggers within the fabric_cicd package. Currently only supports DEBUG.</p> <code>deploy_with_config</code> <p>Deploy items using YAML configuration file with environment-specific settings.</p> <code>publish_all_items</code> <p>Publishes all items defined in the <code>item_type_in_scope</code> list of the given FabricWorkspace object.</p> <code>unpublish_all_orphan_items</code> <p>Unpublishes all orphaned items not present in the repository except for those matching the exclude regex.</p>"},{"location":"code_reference/#fabric_cicd.FabricWorkspace","title":"FabricWorkspace","text":"<pre><code>FabricWorkspace(\n    repository_directory: str,\n    item_type_in_scope: Optional[list[str]] = None,\n    environment: str = \"N/A\",\n    workspace_id: Optional[str] = None,\n    workspace_name: Optional[str] = None,\n    token_credential: TokenCredential = None,\n    **kwargs,\n)\n</code></pre> <p>A class to manage and publish workspace items to the Fabric API.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>Optional[str]</code> <p>The ID of the workspace to interact with. Either <code>workspace_id</code> or <code>workspace_name</code> must be provided. Considers only <code>workspace_id</code> if both are specified.</p> <code>None</code> <code>workspace_name</code> <code>Optional[str]</code> <p>The name of the workspace to interact with. Either <code>workspace_id</code> or <code>workspace_name</code> must be provided. Considers only <code>workspace_id</code> if both are specified.</p> <code>None</code> <code>repository_directory</code> <code>str</code> <p>Local directory path of the repository where items are to be deployed from.</p> required <code>item_type_in_scope</code> <code>Optional[list[str]]</code> <p>Item types that should be deployed for a given workspace. If omitted, defaults to all available item types.</p> <code>None</code> <code>environment</code> <code>str</code> <p>The environment to be used for parameterization.</p> <code>'N/A'</code> <code>token_credential</code> <code>TokenCredential</code> <p>The token credential to use for API requests.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Examples:</p> <p>Basic usage</p> <pre><code>&gt;&gt;&gt; from fabric_cicd import FabricWorkspace\n&gt;&gt;&gt; workspace = FabricWorkspace(\n...     workspace_id=\"your-workspace-id\",\n...     repository_directory=\"/path/to/repo\",\n...     item_type_in_scope=[\"Environment\", \"Notebook\", \"DataPipeline\"]\n... )\n</code></pre> <p>Basic usage with workspace_name</p> <pre><code>&gt;&gt;&gt; from fabric_cicd import FabricWorkspace\n&gt;&gt;&gt; workspace = FabricWorkspace(\n...     workspace_name=\"your-workspace-name\",\n...     repository_directory=\"/path/to/repo\"\n... )\n</code></pre> <p>With optional parameters</p> <pre><code>&gt;&gt;&gt; from fabric_cicd import FabricWorkspace\n&gt;&gt;&gt; workspace = FabricWorkspace(\n...     workspace_id=\"your-workspace-id\",\n...     repository_directory=\"/your/path/to/repo\",\n...     item_type_in_scope=[\"Environment\", \"Notebook\", \"DataPipeline\"],\n...     environment=\"your-target-environment\"\n... )\n</code></pre> <p>With token credential</p> <pre><code>&gt;&gt;&gt; from fabric_cicd import FabricWorkspace\n&gt;&gt;&gt; from azure.identity import ClientSecretCredential\n&gt;&gt;&gt; client_id = \"your-client-id\"\n&gt;&gt;&gt; client_secret = \"your-client-secret\"\n&gt;&gt;&gt; tenant_id = \"your-tenant-id\"\n&gt;&gt;&gt; token_credential = ClientSecretCredential(\n...     client_id=client_id, client_secret=client_secret, tenant_id=tenant_id\n... )\n&gt;&gt;&gt; workspace = FabricWorkspace(\n...     workspace_id=\"your-workspace-id\",\n...     repository_directory=\"/your/path/to/repo\",\n...     item_type_in_scope=[\"Environment\", \"Notebook\", \"DataPipeline\"],\n...     token_credential=token_credential\n... )\n</code></pre>"},{"location":"code_reference/#fabric_cicd.FabricWorkspace.base_api_url","title":"base_api_url  <code>property</code>","text":"<pre><code>base_api_url: str\n</code></pre> <p>Construct the base API URL using constants.</p>"},{"location":"code_reference/#fabric_cicd.append_feature_flag","title":"append_feature_flag","text":"<pre><code>append_feature_flag(feature: str) -&gt; None\n</code></pre> <p>Append a feature flag to the global feature_flag set.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>str</code> <p>The feature flag to be included.</p> required <p>Examples:</p> <p>Basic usage</p> <pre><code>&gt;&gt;&gt; from fabric_cicd import append_feature_flag\n&gt;&gt;&gt; append_feature_flag(\"enable_lakehouse_unpublish\")\n</code></pre>"},{"location":"code_reference/#fabric_cicd.change_log_level","title":"change_log_level","text":"<pre><code>change_log_level(level: str = 'DEBUG') -&gt; None\n</code></pre> <p>Sets the log level for all loggers within the fabric_cicd package. Currently only supports DEBUG.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>The logging level to set (e.g., DEBUG).</p> <code>'DEBUG'</code> <p>Examples:</p> <p>Basic usage</p> <pre><code>&gt;&gt;&gt; from fabric_cicd import change_log_level\n&gt;&gt;&gt; change_log_level(\"DEBUG\")\n</code></pre>"},{"location":"code_reference/#fabric_cicd.deploy_with_config","title":"deploy_with_config","text":"<pre><code>deploy_with_config(\n    config_file_path: str,\n    environment: str = \"N/A\",\n    token_credential: Optional[TokenCredential] = None,\n    config_override: Optional[dict] = None,\n) -&gt; None\n</code></pre> <p>Deploy items using YAML configuration file with environment-specific settings. This function provides a simplified deployment interface that loads configuration from a YAML file and executes deployment operations based on environment-specific settings. It constructs the necessary FabricWorkspace object internally and handles publish/unpublish operations according to the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config_file_path</code> <code>str</code> <p>Path to the YAML configuration file as a string.</p> required <code>environment</code> <code>str</code> <p>Environment name to use for deployment (e.g., 'dev', 'test', 'prod'), if missing defaults to 'N/A'.</p> <code>'N/A'</code> <code>token_credential</code> <code>Optional[TokenCredential]</code> <p>Optional Azure token credential for authentication.</p> <code>None</code> <code>config_override</code> <code>Optional[dict]</code> <p>Optional dictionary to override specific configuration values.</p> <code>None</code> <p>Raises:</p> Type Description <code>InputError</code> <p>If configuration file is invalid or environment not found.</p> <code>FileNotFoundError</code> <p>If configuration file doesn't exist.</p> <p>Examples:</p> <p>Basic usage</p> <pre><code>&gt;&gt;&gt; from fabric_cicd import deploy_with_config\n&gt;&gt;&gt; deploy_with_config(\n...     config_file_path=\"workspace/config.yml\",\n...     environment=\"prod\"\n... )\n</code></pre> <p>With custom authentication</p> <pre><code>&gt;&gt;&gt; from fabric_cicd import deploy_with_config\n&gt;&gt;&gt; from azure.identity import ClientSecretCredential\n&gt;&gt;&gt; credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n&gt;&gt;&gt; deploy_with_config(\n...     config_file_path=\"workspace/config.yml\",\n...     environment=\"prod\",\n...     token_credential=credential\n... )\n</code></pre> <p>With override configuration</p> <pre><code>&gt;&gt;&gt; from fabric_cicd import deploy_with_config\n&gt;&gt;&gt; from azure.identity import ClientSecretCredential\n&gt;&gt;&gt; credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n&gt;&gt;&gt; deploy_with_config(\n...     config_file_path=\"workspace/config.yml\",\n...     environment=\"prod\",\n...     config_override={\n...         \"core\": {\n...             \"item_types_in_scope\": [\"Notebook\"]\n...         },\n...         \"publish\": {\n...             \"skip\": {\n...                 \"prod\": False\n...             }\n...         }\n...     }\n... )\n</code></pre>"},{"location":"code_reference/#fabric_cicd.publish_all_items","title":"publish_all_items","text":"<pre><code>publish_all_items(\n    fabric_workspace_obj: FabricWorkspace,\n    item_name_exclude_regex: Optional[str] = None,\n    items_to_include: Optional[list[str]] = None,\n) -&gt; None\n</code></pre> <p>Publishes all items defined in the <code>item_type_in_scope</code> list of the given FabricWorkspace object.</p> <p>Parameters:</p> Name Type Description Default <code>fabric_workspace_obj</code> <code>FabricWorkspace</code> <p>The FabricWorkspace object containing the items to be published.</p> required <code>item_name_exclude_regex</code> <code>Optional[str]</code> <p>Regex pattern to exclude specific items from being published.</p> <code>None</code> <code>items_to_include</code> <code>Optional[list[str]]</code> <p>List of items in the format \"item_name.item_type\" that should be published.</p> <code>None</code> items_to_include <p>This is an experimental feature in fabric-cicd. Use at your own risk as selective deployments are not recommended due to item dependencies. To enable this feature, see How To -&gt; Optional Features for information on which flags to enable.</p> <p>Examples:</p> <p>Basic usage</p> <pre><code>&gt;&gt;&gt; from fabric_cicd import FabricWorkspace, publish_all_items\n&gt;&gt;&gt; workspace = FabricWorkspace(\n...     workspace_id=\"your-workspace-id\",\n...     repository_directory=\"/path/to/repo\",\n...     item_type_in_scope=[\"Environment\", \"Notebook\", \"DataPipeline\"]\n... )\n&gt;&gt;&gt; publish_all_items(workspace)\n</code></pre> <p>With regex name exclusion</p> <pre><code>&gt;&gt;&gt; from fabric_cicd import FabricWorkspace, publish_all_items\n&gt;&gt;&gt; workspace = FabricWorkspace(\n...     workspace_id=\"your-workspace-id\",\n...     repository_directory=\"/path/to/repo\",\n...     item_type_in_scope=[\"Environment\", \"Notebook\", \"DataPipeline\"]\n... )\n&gt;&gt;&gt; exclude_regex = \".*_do_not_publish\"\n&gt;&gt;&gt; publish_all_items(workspace, item_name_exclude_regex=exclude_regex)\n</code></pre> <p>With items to include</p> <pre><code>&gt;&gt;&gt; from fabric_cicd import FabricWorkspace, publish_all_items\n&gt;&gt;&gt; workspace = FabricWorkspace(\n...     workspace_id=\"your-workspace-id\",\n...     repository_directory=\"/path/to/repo\",\n...     item_type_in_scope=[\"Environment\", \"Notebook\", \"DataPipeline\"]\n... )\n&gt;&gt;&gt; items_to_include = [\"Hello World.Notebook\", \"Hello.Environment\"]\n&gt;&gt;&gt; publish_all_items(workspace, items_to_include=items_to_include)\n</code></pre>"},{"location":"code_reference/#fabric_cicd.unpublish_all_orphan_items","title":"unpublish_all_orphan_items","text":"<pre><code>unpublish_all_orphan_items(\n    fabric_workspace_obj: FabricWorkspace,\n    item_name_exclude_regex: str = \"^$\",\n    items_to_include: Optional[list[str]] = None,\n) -&gt; None\n</code></pre> <p>Unpublishes all orphaned items not present in the repository except for those matching the exclude regex.</p> <p>Parameters:</p> Name Type Description Default <code>fabric_workspace_obj</code> <code>FabricWorkspace</code> <p>The FabricWorkspace object containing the items to be published.</p> required <code>item_name_exclude_regex</code> <code>str</code> <p>Regex pattern to exclude specific items from being unpublished. Default is '^$' which will exclude nothing.</p> <code>'^$'</code> <code>items_to_include</code> <code>Optional[list[str]]</code> <p>List of items in the format \"item_name.item_type\" that should be unpublished.</p> <code>None</code> items_to_include <p>This is an experimental feature in fabric-cicd. Use at your own risk as selective unpublishing is not recommended due to item dependencies. To enable this feature, see How To -&gt; Optional Features for information on which flags to enable.</p> <p>Examples:</p> <p>Basic usage</p> <pre><code>&gt;&gt;&gt; from fabric_cicd import FabricWorkspace, publish_all_items, unpublish_all_orphan_items\n&gt;&gt;&gt; workspace = FabricWorkspace(\n...     workspace_id=\"your-workspace-id\",\n...     repository_directory=\"/path/to/repo\",\n...     item_type_in_scope=[\"Environment\", \"Notebook\", \"DataPipeline\"]\n... )\n&gt;&gt;&gt; publish_all_items(workspace)\n&gt;&gt;&gt; unpublish_orphaned_items(workspace)\n</code></pre> <p>With regex name exclusion</p> <pre><code>&gt;&gt;&gt; from fabric_cicd import FabricWorkspace, publish_all_items, unpublish_all_orphan_items\n&gt;&gt;&gt; workspace = FabricWorkspace(\n...     workspace_id=\"your-workspace-id\",\n...     repository_directory=\"/path/to/repo\",\n...     item_type_in_scope=[\"Environment\", \"Notebook\", \"DataPipeline\"]\n... )\n&gt;&gt;&gt; publish_all_items(workspace)\n&gt;&gt;&gt; exclude_regex = \".*_do_not_delete\"\n&gt;&gt;&gt; unpublish_orphaned_items(workspace, item_name_exclude_regex=exclude_regex)\n</code></pre> <p>With items to include</p> <pre><code>&gt;&gt;&gt; from fabric_cicd import FabricWorkspace, publish_all_items, unpublish_all_orphan_items\n&gt;&gt;&gt; workspace = FabricWorkspace(\n...     workspace_id=\"your-workspace-id\",\n...     repository_directory=\"/path/to/repo\",\n...     item_type_in_scope=[\"Environment\", \"Notebook\", \"DataPipeline\"]\n... )\n&gt;&gt;&gt; publish_all_items(workspace)\n&gt;&gt;&gt; items_to_include = [\"Hello World.Notebook\", \"Run Hello World.DataPipeline\"]\n&gt;&gt;&gt; unpublish_orphaned_items(workspace, items_to_include=items_to_include)\n</code></pre>"},{"location":"contribution/","title":"Contribution","text":"<p>This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.</p> <p>When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.</p> <p>This project has adopted the Microsoft Open Source Code of Conduct.</p>"},{"location":"contribution/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python (see Installation for version requirements)</li> <li>PowerShell</li> <li>Azure CLI or Az.Accounts PowerShell module</li> <li>Visual Studio Code (VS Code)</li> </ul>"},{"location":"contribution/#initial-configuration","title":"Initial Configuration","text":"<ol> <li> <p>Fork the Repository on GitHub:</p> <ul> <li>Go to the repository fabric-cicd on GitHub.</li> <li>In the top right corner, click on the Fork button.</li> <li>This will create a copy of the repository in your own GitHub account.</li> </ul> </li> <li> <p>Clone Your Forked Repository:</p> <ul> <li>Once the fork is complete, go to your GitHub account and open the forked repository.</li> <li>Click on the Code button, and clone to VS Code.</li> </ul> </li> <li> <p>Run activate.ps1:</p> <ul> <li>Open the Project in VS Code</li> <li>Open PowerShell terminal</li> <li>Run activate.ps1 which will install uv, and ruff if not already found. And set up the default environment leveraging uv sync.     <pre><code>.\\activate.ps1\n</code></pre> Note that this is technically optional and is designed to work with PowerShell. You can execute these steps manually as well, this is merely a helper</li> </ul> </li> <li> <p>Select Python Interpreter:</p> <ul> <li>Open the Command Palette (Ctrl+Shift+P) and select <code>Python: Select Interpreter</code>.</li> <li>Choose the interpreter from the <code>venv</code> directory.</li> </ul> </li> <li> <p>Ensure All VS Code Extensions Are Installed:</p> <ul> <li>Open the Command Palette (Ctrl+Shift+P) and select <code>Extensions: Show Recommended Extensions</code>.</li> <li>Install all extensions recommended for the workspace.</li> </ul> </li> </ol>"},{"location":"contribution/#development","title":"Development","text":""},{"location":"contribution/#managing-dependencies","title":"Managing Dependencies","text":"<ul> <li>All dependencies in this project are managed by uv which will resolve all dependencies and lock the versions to speed up virtual environment creation.</li> <li>For additions, run:     <pre><code>uv add &lt;package-name&gt;\n</code></pre></li> <li>For removals, run:     <pre><code>uv remove &lt;package-name&gt;\n</code></pre></li> </ul>"},{"location":"contribution/#code-formatting-linting","title":"Code Formatting &amp; Linting","text":"<ul> <li>The python code within this project is maintained by ruff.</li> <li>If you install the recommended extensions, ruff will auto format on save of any file.</li> <li>Before being able to merge a PR, ruff is ran in a Github Action to ensure the files are properly formatted and maintained.</li> <li>To force linting, run the following.     <pre><code>ruff format\nruff check\n</code></pre></li> </ul>"},{"location":"contribution/#pull-request-requirements","title":"Pull Request Requirements","text":"<ul> <li>All pull requests must be linked to an issue. This ensures proper tracking and context for changes.</li> <li>PR Title MUST follow this exact format: \"Fixes #123 - Short Description\" where #123 is the issue number<ul> <li>Use \"Fixes\" for bug fixes, \"Closes\" for features, \"Resolves\" for other changes</li> <li>Example: \"Fixes #520 - Add Python version requirements to documentation\"</li> <li>Version bump PRs are an exception: title must be \"vX.X.X\" format only</li> </ul> </li> <li>Before creating a pull request:<ol> <li>Create or identify an existing issue that describes the problem, feature request, or change you're addressing.</li> <li>Use the proper PR title format as specified above to link to the issue.</li> <li>You can also reference issues in commit messages using the same format.</li> </ol> </li> <li>GitHub Actions will automatically check that your PR is linked to a valid issue and will fail if no valid reference is found.</li> <li>If you need to create an issue, use the appropriate template:<ul> <li>Bug Report</li> <li>Feature Request </li> <li>Documentation</li> <li>Question</li> <li>Technical Debt</li> </ul> </li> </ul>"},{"location":"example/","title":"How To","text":"<p>Welcome to the Examples section! Here you will find all necessary code samples to leverage fabric-cicd. If there is any missing information, raise a documentation issue on GitHub.</p>"},{"location":"example/#contents","title":"Contents","text":"<ul> <li>Authentication Examples<ul> <li>Default Credential</li> <li>CLI Credential</li> <li>AZ PowerShell Credential</li> <li>Explicit SPN Secret Credential</li> </ul> </li> <li>Deployment Variable Examples<ul> <li>Branch Based</li> <li>Passed Arguments</li> </ul> </li> <li>Release Pipeline Examples<ul> <li>Azure CLI</li> <li>Azure PowerShell</li> <li>Variable Groups</li> </ul> </li> </ul>"},{"location":"example/authentication/","title":"Authentication Examples","text":"<p>The following are the most common authentication flows for fabric-cicd. However, because fabric-cicd supports any TokenCredential, there are multiple authentication methods available beyond the ones described here.</p>"},{"location":"example/authentication/#default-credential","title":"Default Credential","text":"<p>This approach utilizes the default credential flow, meaning no explicit TokenCredential is provided. It is the most common authentication method and is particularly useful with deployments where authentication is defined outside of this execution.</p> LocalAzure DevOpsGitHub <pre><code>'''Log in with Azure CLI (az login) or Azure PowerShell (Connect-AzAccount) prior to execution'''\n\nfrom pathlib import Path\n\nfrom fabric_cicd import FabricWorkspace, publish_all_items, unpublish_all_orphan_items\n\n# Assumes your script is one level down from root\nroot_directory = Path(__file__).resolve().parent\n\n# Sample values for FabricWorkspace parameters\nworkspace_id = \"your-workspace-id\"\nenvironment = \"your-environment\"\nrepository_directory = str(root_directory / \"your-workspace-directory\")\nitem_type_in_scope = [\"Notebook\", \"DataPipeline\", \"Environment\"]\n\n# Initialize the FabricWorkspace object with the required parameters\ntarget_workspace = FabricWorkspace(\n    workspace_id=workspace_id,\n    environment=environment,\n    repository_directory=repository_directory,\n    item_type_in_scope=item_type_in_scope,\n)\n\n# Publish all items defined in item_type_in_scope\npublish_all_items(target_workspace)\n\n# Unpublish all items defined in item_type_in_scope not found in repository\nunpublish_all_orphan_items(target_workspace)\n</code></pre> <pre><code>'''\nLog in with Azure CLI (az login) or Azure PowerShell (Connect-AzAccount) prior to execution\nOR (Preferred) Use Az CLI or AzPowerShell ADO Tasks with a Service Connection\n'''\n\nimport sys\nimport os\nfrom pathlib import Path\n\nfrom fabric_cicd import FabricWorkspace, publish_all_items, unpublish_all_orphan_items, change_log_level\n\n# Force unbuffered output like `python -u`\nsys.stdout.reconfigure(line_buffering=True, write_through=True)\nsys.stderr.reconfigure(line_buffering=True, write_through=True)\n\n# Enable debugging if defined in Azure DevOps pipeline\nif os.getenv(\"SYSTEM_DEBUG\", \"false\").lower() == \"true\":\n    change_log_level(\"DEBUG\")\n\n# Assumes your script is one level down from root\nroot_directory = Path(__file__).resolve().parent\n\n# Sample values for FabricWorkspace parameters\nworkspace_id = \"your-workspace-id\"\nenvironment = \"your-environment\"\nrepository_directory = str(root_directory / \"your-workspace-directory\")\nitem_type_in_scope = [\"Notebook\", \"DataPipeline\", \"Environment\"]\n\n# Initialize the FabricWorkspace object with the required parameters\ntarget_workspace = FabricWorkspace(\n    workspace_id=workspace_id,\n    environment=environment,\n    repository_directory=repository_directory,\n    item_type_in_scope=item_type_in_scope,\n)\n\n# Publish all items defined in item_type_in_scope\npublish_all_items(target_workspace)\n\n# Unpublish all items defined in item_type_in_scope not found in repository\nunpublish_all_orphan_items(target_workspace)\n</code></pre> <pre><code>'''Unconfirmed example at this time, however, the Azure DevOps example is a good starting point'''\n</code></pre>"},{"location":"example/authentication/#cli-credential","title":"CLI Credential","text":"<p>This approach utilizes the CLI credential flow, meaning it only refers to the authentication established with az login. This is agnostic of the executing user, it can be UPN, SPN, Managed Identity, etc. Whatever is used to log in will be used.</p> LocalAzure DevOpsGitHub <pre><code>'''Log in with Azure CLI (az login) prior to execution'''\n\nfrom pathlib import Path\n\nfrom azure.identity import AzureCliCredential\nfrom fabric_cicd import FabricWorkspace, publish_all_items, unpublish_all_orphan_items\n\n# Assumes your script is one level down from root\nroot_directory = Path(__file__).resolve().parent\n\n# Sample values for FabricWorkspace parameters\nworkspace_id = \"your-workspace-id\"\nenvironment = \"your-environment\"\nrepository_directory = str(root_directory / \"your-workspace-directory\")\nitem_type_in_scope = [\"Notebook\", \"DataPipeline\", \"Environment\"]\n\n# Use Azure CLI credential to authenticate\ntoken_credential = AzureCliCredential()\n\n# Initialize the FabricWorkspace object with the required parameters\ntarget_workspace = FabricWorkspace(\n    workspace_id=workspace_id,\n    environment=environment,\n    repository_directory=repository_directory,\n    item_type_in_scope=item_type_in_scope,\n    token_credential=token_credential,\n)\n\n# Publish all items defined in item_type_in_scope\npublish_all_items(target_workspace)\n\n# Unpublish all items defined in item_type_in_scope not found in repository\nunpublish_all_orphan_items(target_workspace)\n</code></pre> <pre><code>'''\nLog in with Azure CLI (az login) prior to execution\nOR (Preferred) Use Az CLI ADO Tasks with a Service Connection\n'''\n\nimport sys\nimport os\nfrom pathlib import Path\n\nfrom azure.identity import AzureCliCredential\nfrom fabric_cicd import FabricWorkspace, publish_all_items, unpublish_all_orphan_items, change_log_level\n\n# Force unbuffered output like `python -u`\nsys.stdout.reconfigure(line_buffering=True, write_through=True)\nsys.stderr.reconfigure(line_buffering=True, write_through=True)\n\n# Enable debugging if defined in Azure DevOps pipeline\nif os.getenv(\"SYSTEM_DEBUG\", \"false\").lower() == \"true\":\n    change_log_level(\"DEBUG\")\n\n# Assumes your script is one level down from root\nroot_directory = Path(__file__).resolve().parent\n\n# Sample values for FabricWorkspace parameters\nworkspace_id = \"your-workspace-id\"\nenvironment = \"your-environment\"\nrepository_directory = str(root_directory / \"your-workspace-directory\")\nitem_type_in_scope = [\"Notebook\", \"DataPipeline\", \"Environment\"]\n\n# Use Azure CLI credential to authenticate\ntoken_credential = AzureCliCredential()\n\n# Initialize the FabricWorkspace object with the required parameters\ntarget_workspace = FabricWorkspace(\n    workspace_id=workspace_id,\n    environment=environment,\n    repository_directory=repository_directory,\n    item_type_in_scope=item_type_in_scope,\n    token_credential=token_credential,\n)\n\n# Publish all items defined in item_type_in_scope\npublish_all_items(target_workspace)\n\n# Unpublish all items defined in item_type_in_scope not found in repository\nunpublish_all_orphan_items(target_workspace)\n</code></pre> <pre><code>'''Unconfirmed example at this time, however, the Azure DevOps example is a good starting point'''\n</code></pre>"},{"location":"example/authentication/#az-powershell-credential","title":"AZ PowerShell Credential","text":"<p>This approach utilizes the AZ PowerShell credential flow, meaning it only refers to the authentication established with Connect-AzAccount. This is agnostic of the executing user, it can be UPN, SPN, Managed Identity, etc. Whatever is used to log in will be used.</p> LocalAzure DevOpsGitHub <pre><code>'''Log in with Azure PowerShell (Connect-AzAccount) prior to execution'''\n\nfrom pathlib import Path\n\nfrom azure.identity import AzurePowerShellCredential\nfrom fabric_cicd import FabricWorkspace, publish_all_items, unpublish_all_orphan_items\n\n# Assumes your script is one level down from root\nroot_directory = Path(__file__).resolve().parent\n\n# Sample values for FabricWorkspace parameters\nworkspace_id = \"your-workspace-id\"\nenvironment = \"your-environment\"\nrepository_directory = str(root_directory / \"your-workspace-directory\")\nitem_type_in_scope = [\"Notebook\", \"DataPipeline\", \"Environment\"]\n\n# Use Azure CLI credential to authenticate\ntoken_credential = AzurePowerShellCredential()\n\n# Initialize the FabricWorkspace object with the required parameters\ntarget_workspace = FabricWorkspace(\n    workspace_id=workspace_id,\n    environment=environment,\n    repository_directory=repository_directory,\n    item_type_in_scope=item_type_in_scope,\n    token_credential=token_credential,\n)\n\n# Publish all items defined in item_type_in_scope\npublish_all_items(target_workspace)\n\n# Unpublish all items defined in item_type_in_scope not found in repository\nunpublish_all_orphan_items(target_workspace)\n</code></pre> <pre><code>'''\nLog in with Azure PowerShell (Connect-AzAccount) prior to execution\nOR (Preferred) Use AzPowerShell ADO Tasks with a Service Connection\n'''\n\nimport sys\nimport os\nfrom pathlib import Path\n\nfrom azure.identity import AzurePowerShellCredential\nfrom fabric_cicd import FabricWorkspace, publish_all_items, unpublish_all_orphan_items, change_log_level\n\n# Force unbuffered output like `python -u`\nsys.stdout.reconfigure(line_buffering=True, write_through=True)\nsys.stderr.reconfigure(line_buffering=True, write_through=True)\n\n# Enable debugging if defined in Azure DevOps pipeline\nif os.getenv(\"SYSTEM_DEBUG\", \"false\").lower() == \"true\":\n    change_log_level(\"DEBUG\")\n\n# Assumes your script is one level down from root\nroot_directory = Path(__file__).resolve().parent\n\n# Sample values for FabricWorkspace parameters\nworkspace_id = \"your-workspace-id\"\nenvironment = \"your-environment\"\nrepository_directory = str(root_directory / \"your-workspace-directory\")\nitem_type_in_scope = [\"Notebook\", \"DataPipeline\", \"Environment\"]\n\n# Use Azure CLI credential to authenticate\ntoken_credential = AzurePowerShellCredential()\n\n# Initialize the FabricWorkspace object with the required parameters\ntarget_workspace = FabricWorkspace(\n    workspace_id=workspace_id,\n    environment=environment,\n    repository_directory=repository_directory,\n    item_type_in_scope=item_type_in_scope,\n    token_credential=token_credential,\n)\n\n# Publish all items defined in item_type_in_scope\npublish_all_items(target_workspace)\n\n# Unpublish all items defined in item_type_in_scope not found in repository\nunpublish_all_orphan_items(target_workspace)\n</code></pre> <pre><code>'''\nUnconfirmed example at this time, however, the Azure DevOps example is a good starting point\n'''\n</code></pre>"},{"location":"example/authentication/#explicit-spn-secret-credential","title":"Explicit SPN Secret Credential","text":"<p>This approach utilizes directly passing in SPN Client Id and Client Secret. Although you can pass in directly, it's not recommended and should store this outside of your code. It's important to consider that SPN + Secret is still possible to leverage in the above AZ PowerShell and AZ CLI flows</p> LocalAzure DevOpsGitHub <pre><code>'''Pass the required SPN values directly into the credential object, does not require AZ PowerShell or AZ CLI'''\n\nfrom pathlib import Path\n\nfrom azure.identity import ClientSecretCredential\nfrom fabric_cicd import FabricWorkspace, publish_all_items, unpublish_all_orphan_items\n\n# Assumes your script is one level down from root\nroot_directory = Path(__file__).resolve().parent\n\n# Sample values for FabricWorkspace parameters\nworkspace_id = \"your-workspace-id\"\nenvironment = \"your-environment\"\nrepository_directory = str(root_directory / \"your-workspace-directory\")\nitem_type_in_scope = [\"Notebook\", \"DataPipeline\", \"Environment\"]\n\n# Use Azure CLI credential to authenticate\nclient_id = \"your-client-id\"\nclient_secret = \"your-client-secret\"\ntenant_id = \"your-tenant-id\"\ntoken_credential = ClientSecretCredential(client_id=client_id, client_secret=client_secret, tenant_id=tenant_id)\n\n# Initialize the FabricWorkspace object with the required parameters\ntarget_workspace = FabricWorkspace(\n    workspace_id=workspace_id,\n    environment=environment,\n    repository_directory=repository_directory,\n    item_type_in_scope=item_type_in_scope,\n    token_credential=token_credential,\n)\n\n# Publish all items defined in item_type_in_scope\npublish_all_items(target_workspace)\n\n# Unpublish all items defined in item_type_in_scope not found in repository\nunpublish_all_orphan_items(target_workspace)\n</code></pre> <pre><code>'''\nPass the required SPN values directly into the credential object\nOR Store the SPN Secret in Key Vault and reference key vault in Python\nOR Store the SPN Secret in Key Vault, link key vault to ADO variable group, and reference variable group environment variable in Python\nOR (Preferred) Use AZ CLI or AZ PowerShell task and leverage Service Connection (defined above)\n\n'''\n\nimport sys\nimport os\nfrom pathlib import Path\n\nfrom azure.identity import ClientSecretCredential\nfrom fabric_cicd import FabricWorkspace, publish_all_items, unpublish_all_orphan_items, change_log_level\n\n# Force unbuffered output like `python -u`\nsys.stdout.reconfigure(line_buffering=True, write_through=True)\nsys.stderr.reconfigure(line_buffering=True, write_through=True)\n\n# Enable debugging if defined in Azure DevOps pipeline\nif os.getenv(\"SYSTEM_DEBUG\", \"false\").lower() == \"true\":\n    change_log_level(\"DEBUG\")\n\n# Assumes your script is one level down from root\nroot_directory = Path(__file__).resolve().parent\n\n# Sample values for FabricWorkspace parameters\nworkspace_id = \"your-workspace-id\"\nenvironment = \"your-environment\"\nrepository_directory = str(root_directory / \"your-workspace-directory\")\nitem_type_in_scope = [\"Notebook\", \"DataPipeline\", \"Environment\"]\n\n# Use Azure CLI credential to authenticate\nclient_id = \"your-client-id\"\nclient_secret = \"your-client-secret\"\ntenant_id = \"your-tenant-id\"\ntoken_credential = ClientSecretCredential(client_id=client_id, client_secret=client_secret, tenant_id=tenant_id)\n\n# Initialize the FabricWorkspace object with the required parameters\ntarget_workspace = FabricWorkspace(\n    workspace_id=workspace_id,\n    environment=environment,\n    repository_directory=repository_directory,\n    item_type_in_scope=item_type_in_scope,\n    token_credential=token_credential,\n)\n\n# Publish all items defined in item_type_in_scope\npublish_all_items(target_workspace)\n\n# Unpublish all items defined in item_type_in_scope not found in repository\nunpublish_all_orphan_items(target_workspace)\n</code></pre> <pre><code>'''Unconfirmed example at this time, however, the Azure DevOps example is a good starting point'''\n</code></pre>"},{"location":"example/deployment_variable/","title":"Deployment Variable Examples","text":"<p>A key concept in CI/CD is defining environment-specific deployment variables. The following are examples on how to inject variables from outside of the python script to handle values that are environment specific, or common accross other tooling.</p>"},{"location":"example/deployment_variable/#branch-based","title":"Branch Based","text":"<p>Leverage the following when you have specific values that you need to define per branch you are deploying from.</p> LocalAzure DevOpsGitHub <pre><code>'''Leverages Default Credential Flow for authentication. Determines variables based on locally checked out branch.'''\n\nfrom pathlib import Path\n\nimport git  # Depends on pip install gitpython\nfrom fabric_cicd import FabricWorkspace, publish_all_items, unpublish_all_orphan_items\n\n# Assumes your script is one level down from root\nroot_directory = Path(__file__).resolve().parent\n\nrepo = git.Repo(root_directory)\nrepo.remotes.origin.pull()\nbranch = repo.active_branch.name\n\n# The defined environment values should match the names found in the parameter.yml file\nif branch == \"dev\":\n    workspace_id = \"dev-workspace-id\"\n    environment = \"DEV\"\nelif branch == \"main\":\n    workspace_id = \"prod-workspace-id\"\n    environment = \"PROD\"\nelse:\n    raise ValueError(\"Invalid branch to deploy from\")\n\n# Sample values for FabricWorkspace parameters\nrepository_directory = str(root_directory / \"your-workspace-directory\")\nitem_type_in_scope = [\"Notebook\", \"DataPipeline\", \"Environment\"]\n\n# Initialize the FabricWorkspace object with the required parameters\ntarget_workspace = FabricWorkspace(\n    workspace_id=workspace_id,\n    environment=environment,\n    repository_directory=repository_directory,\n    item_type_in_scope=item_type_in_scope,\n)\n\n# Publish all items defined in item_type_in_scope\npublish_all_items(target_workspace)\n\n# Unpublish all items defined in item_type_in_scope not found in repository\nunpublish_all_orphan_items(target_workspace)\n</code></pre> <pre><code>'''Leverages Default Credential Flow for authentication. Determines variables based on the branch that originated the build.'''\n\nimport sys\nimport os\nfrom pathlib import Path\n\nfrom fabric_cicd import FabricWorkspace, publish_all_items, unpublish_all_orphan_items, change_log_level\n\n# Force unbuffered output like `python -u`\nsys.stdout.reconfigure(line_buffering=True, write_through=True)\nsys.stderr.reconfigure(line_buffering=True, write_through=True)\n\n# Enable debugging if defined in Azure DevOps pipeline\nif os.getenv(\"SYSTEM_DEBUG\", \"false\").lower() == \"true\":\n    change_log_level(\"DEBUG\")\n\n# Assumes your script is one level down from root\nroot_directory = Path(__file__).resolve().parent\n\nbranch = os.getenv(\"BUILD_SOURCEBRANCHNAME\")\n\n# The defined environment values should match the names found in the parameter.yml file\nif branch == \"dev\":\n    workspace_id = \"dev-workspace-id\"\n    environment = \"DEV\"\nelif branch == \"main\":\n    workspace_id = \"prod-workspace-id\"\n    environment = \"PROD\"\nelse:\n    raise ValueError(\"Invalid branch to deploy from\")\n\n# Sample values for FabricWorkspace parameters\nrepository_directory = str(root_directory / \"your-workspace-directory\")\nitem_type_in_scope = [\"Notebook\", \"DataPipeline\", \"Environment\"]\n\n\n# Initialize the FabricWorkspace object with the required parameters\ntarget_workspace = FabricWorkspace(\n    workspace_id=workspace_id,\n    environment=environment,\n    repository_directory=repository_directory,\n    item_type_in_scope=item_type_in_scope,\n)\n\n# Publish all items defined in item_type_in_scope\npublish_all_items(target_workspace)\n\n# Unpublish all items defined in item_type_in_scope not found in repository\nunpublish_all_orphan_items(target_workspace)\n</code></pre> <pre><code>'''Unconfirmed example at this time, however, the Azure DevOps example is a good starting point'''\n</code></pre>"},{"location":"example/deployment_variable/#passed-arguments","title":"Passed Arguments","text":"<p>Leverage the following when you want to pass in variables outside of the python script. This is most common for scenarios where you want to use one py script, but have multiple deployments.</p> LocalAzure DevOpsGitHub <pre><code>'''Leverages Default Credential Flow for authentication. Accepts parameters passed into Python during execution.'''\n\nimport argparse\n\nfrom fabric_cicd import FabricWorkspace, publish_all_items, unpublish_all_orphan_items\n\n# Accept parsed arguments\nparser = argparse.ArgumentParser(description='Process Azure Pipeline arguments.')\nparser.add_argument('--workspace_id', type=str)\nparser.add_argument('--environment', type=str)\nparser.add_argument('--repository_directory', type=str)\nparser.add_argument('--items_in_scope', type=str)\nargs = parser.parse_args()\n\n# Sample values for FabricWorkspace parameters\nworkspace_id = args.workspace_id\nenvironment = args.environment\nrepository_directory = args.repository_directory\nitem_type_in_scope = args.items_in_scope.split(\",\")\n\n# Initialize the FabricWorkspace object with the required parameters\ntarget_workspace = FabricWorkspace(\n    workspace_id=workspace_id,\n    environment=environment,\n    repository_directory=repository_directory,\n    item_type_in_scope=item_type_in_scope,\n)\n\n# Publish all items defined in item_type_in_scope\npublish_all_items(target_workspace)\n\n# Unpublish all items defined in item_type_in_scope not found in repository\nunpublish_all_orphan_items(target_workspace)\n</code></pre> <pre><code>'''Leverages Default Credential Flow for authentication. Accepts parameters passed into Python during execution.'''\n\nimport sys\nimport os\nfrom pathlib import Path\n\nfrom fabric_cicd import FabricWorkspace, publish_all_items, unpublish_all_orphan_items, change_log_level\n\n# Force unbuffered output like `python -u`\nsys.stdout.reconfigure(line_buffering=True, write_through=True)\nsys.stderr.reconfigure(line_buffering=True, write_through=True)\n\n# Enable debugging if defined in Azure DevOps pipeline\nif os.getenv(\"SYSTEM_DEBUG\", \"false\").lower() == \"true\":\n    change_log_level(\"DEBUG\")\n\n# Accept parsed arguments\nparser = argparse.ArgumentParser(description='Process Azure Pipeline arguments.')\nparser.add_argument('--workspace_id', type=str)\nparser.add_argument('--environment', type=str)\nparser.add_argument('--repository_directory', type=str)\nparser.add_argument('--items_in_scope', type=str)\nargs = parser.parse_args()\n\n# Sample values for FabricWorkspace parameters\nworkspace_id = args.workspace_id\nenvironment = args.environment\nrepository_directory = args.repository_directory\nitem_type_in_scope = args.items_in_scope.split(\",\")\n\n# Initialize the FabricWorkspace object with the required parameters\ntarget_workspace = FabricWorkspace(\n    workspace_id=workspace_id,\n    environment=environment,\n    repository_directory=repository_directory,\n    item_type_in_scope=item_type_in_scope,\n)\n\n# Publish all items defined in item_type_in_scope\npublish_all_items(target_workspace)\n\n# Unpublish all items defined in item_type_in_scope not found in repository\nunpublish_all_orphan_items(target_workspace)\n</code></pre> <pre><code>'''Unconfirmed example at this time, however, the Azure DevOps example is a good starting point'''\n</code></pre>"},{"location":"example/release_pipeline/","title":"Release Pipeline Examples","text":"<p>The following are some common examples of how to deploy from tooling like Azure DevOps and GitHub. Note that this is not an exhaustive list, nor is it a recommendation to not use a proper Build/Release stage. These are simplified to show the potential.</p>"},{"location":"example/release_pipeline/#azure-cli","title":"Azure CLI","text":"<p>This approach will work for both the Default Credential Flow and the Azure CLI Credential Flow. However, it is recommended to use the Azure CLI Credential Flow in case there are multiple identities present in the build VM.</p> Azure DevOpsGitHub <pre><code>trigger:\n  branches:\n    include:\n      - dev\n      - main\nstages:\n  - stage: Build_Release\n    jobs:\n      - job: Build\n        pool:\n          vmImage: windows-latest\n        steps:\n          - checkout: self\n          - task: UsePythonVersion@0\n            inputs:\n              versionSpec: '3.12'\n              addToPath: true\n          - script: |\n              pip install fabric-cicd\n            displayName: 'Install fabric-cicd'\n          - task: AzureCLI@2\n            displayName: \"Deploy Fabric Workspace\"\n            inputs:\n              azureSubscription: \"your-service-connection\"\n              scriptType: \"ps\"\n              scriptLocation: \"inlineScript\"\n              inlineScript: |\n                python -u $(System.DefaultWorkingDirectory)/.deploy/fabric_workspace.py\n</code></pre> <pre><code>###Unconfirmed example at this time, however, the Azure DevOps example is a good starting point\n</code></pre>"},{"location":"example/release_pipeline/#azure-powershell","title":"Azure PowerShell","text":"<p>This approach will work for both the Default Credential Flow and the Azure PowerShell Credential Flow. However, it is recommended to use the Azure PowerShell Credential Flow in case there are multiple identities present in the build VM.</p> Azure DevOpsGitHub <pre><code>trigger:\n  branches:\n    include:\n      - dev\n      - main\nstages:\n  - stage: Build_Release\n    jobs:\n      - job: Build\n        pool:\n          vmImage: windows-latest\n        steps:\n          - checkout: self\n          - task: UsePythonVersion@0\n            inputs:\n              versionSpec: '3.12'\n              addToPath: true\n          - script: |\n              pip install fabric-cicd\n            displayName: 'Install fabric-cicd'\n          - task: AzurePowerShell@5\n            displayName: \"Deploy Fabric Workspace\"\n            inputs:\n              azureSubscription: \"your-service-connection\"\n              scriptType: \"InlineScript\"\n              scriptLocation: \"inlineScript\"\n              pwsh: true\n              Inline: |\n                python -u $(System.DefaultWorkingDirectory)/.deploy/fabric_workspace.py\n</code></pre> <pre><code>###Unconfirmed example at this time, however, the Azure DevOps example is a good starting point\n</code></pre>"},{"location":"example/release_pipeline/#variable-groups","title":"Variable Groups","text":"<p>This approach is best suited for the Passed Arguments example found in the Deployment Variable Examples, in combination with the Explicit SPN Credential flow in the Authentication Examples. The goal being to define values within the pipeline (or outside the pipeline in Azure DevOps variable groups) and inject them into the python script. Note this also doesn't take a dependency on PowerShell for those organizations or scenarios that PowerShell is not allowed.</p> Azure DevOpsGitHub <pre><code>trigger:\n  branches:\n    include:\n      - dev\n      - main\n\nparameters:\n- name: items_in_scope\n  displayName: Enter Fabric items to be deployed\n  type: string\n  default: '[\"Notebook\",\"DataPipeline\",\"Environment\"]'\n\nvariables:\n- group: Fabric_Deployment_Group_KeyVault # Linked to Azure Key Vault and contains tenant id, SPN client id, and SPN secret\n- group: Fabric_Deployment_Group  # Contains workspace_name and repository directory name\n\nstages:\n  - stage: Build_Release\n    jobs:\n      - job: Build\n        pool:\n          vmImage: windows-latest\n        steps:\n          - checkout: self\n          - task: UsePythonVersion@0\n            inputs:\n              versionSpec: '3.12'\n              addToPath: true\n          - script: |\n              pip install fabric-cicd\n            displayName: 'Install fabric-cicd'\n          - task: PythonScript@0\n            inputs:\n              scriptSource: 'filePath'\n              scriptPath: '.deploy/fabric_workspace.py'\n              arguments: &gt;-\n                --spn_client_id $(client_id) # from Fabric_Deployment_Group_KeyVault\n                --spn_client_secret $(client_secret) # from Fabric_Deployment_Group_KeyVault\n                --tenant_id $(tenant_id) # from Fabric_Deployment_Group_KeyVault\n                --workspace_id $(workspace_id) # from Fabric_Deployment_Group\n                --environment $(environment_name) # from Fabric_Deployment_Group\n                --repository_directory $repository_directory # from Fabric_Deployment_Group\n                --item_types_in_scope ${{ parameters.items_in_scope }}\n</code></pre> <pre><code>###Unconfirmed example at this time, however, the Azure DevOps example is a good starting point\n</code></pre>"},{"location":"how_to/","title":"How To","text":"<p>Welcome to the How To section! Here you will find all necessary information to leverage fabric-cicd. If there is any missing information, raise a documentation issue on GitHub.</p>"},{"location":"how_to/#contents","title":"Contents","text":"<ul> <li>Getting Started<ul> <li>Installation</li> <li>Authentication</li> <li>Directory Structure</li> <li>GIT Flow</li> </ul> </li> <li>Item Types<ul> <li>Activators</li> <li>API for GraphQL</li> <li>Copy Jobs</li> <li>Dataflows</li> <li>Data Pipelines</li> <li>Environments</li> <li>Eventhouses</li> <li>Eventstreams</li> <li>KQL Databases</li> <li>KQL Querysets</li> <li>Lakehouses</li> <li>Mirrored Databases</li> <li>Notebooks</li> <li>Real-Time Dashboard</li> <li>Reports</li> <li>Semantic Models</li> <li>SQL Databases</li> <li>Variable Libraries</li> <li>Warehouses</li> </ul> </li> <li>Configuration Deployment<ul> <li>Overview</li> <li>Configuration File Setup<ul> <li>Core Settings</li> <li>Publish Settings</li> <li>Unpublish Settings</li> <li>Features Setting</li> <li>Constants Setting</li> <li>Sample Configuration File</li> </ul> </li> <li>Configuration File Deployment<ul> <li>Basic Usage</li> <li>Custom Authentication</li> <li>Configuration Override</li> </ul> </li> <li>Troubleshooting Guide</li> </ul> </li> <li>Parameterization<ul> <li>Overview</li> <li>Parameter Inputs<ul> <li><code>find_replace</code></li> <li><code>key_value_replace</code></li> <li><code>spark_pool</code></li> </ul> </li> <li>Advanced Find and Replace<ul> <li><code>find_value</code> Regex</li> <li>Dynamic Replacement</li> <li>Environment Variable Replacement</li> <li>File Filters</li> <li>_ALL_ Environment Key in <code>replace_value</code></li> </ul> </li> <li>Optional Fields<ul> <li>Regex Pattern Match<ul> <li><code>is_regex</code></li> </ul> </li> <li>Supported File Filters<ul> <li><code>item_type</code></li> <li><code>item_name</code></li> <li><code>file_path</code></li> </ul> </li> </ul> </li> <li>Parameter File Validation</li> <li>Sample Parameter File</li> <li>Examples by Item Type<ul> <li>Notebooks<ul> <li><code>find_replace</code> Parameterization Case</li> <li>Advanced <code>find_replace</code> Parameterization Case</li> </ul> </li> <li>Data Pipelines<ul> <li><code>key_value_replace</code> Parameterization Case</li> </ul> </li> <li>Schedules<ul> <li><code>key_value_replace</code> Parameterization Case</li> </ul> </li> <li>Environments<ul> <li><code>spark_pool</code> Parameterization Case</li> </ul> </li> <li>Dataflows<ul> <li>Parameterization Overview</li> <li>Parameterization Guidance</li> <li>Advanced <code>find_replace</code> Parameterization Case</li> </ul> </li> </ul> </li> </ul> </li> <li>Optional Features<ul> <li>Feature Flags</li> <li>Debugging</li> </ul> </li> </ul>"},{"location":"how_to/config_deployment/","title":"Configuration Deployment","text":""},{"location":"how_to/config_deployment/#overview","title":"Overview","text":"<p>Configuration-based deployment provides an alternative way to manage the deployment of Fabric items across multiple environments. Instead of using the traditional approach of defining a workspace object with various parameters and then running the publish/unpublish functions, this approach centralizes all deployment settings in a single YAML configuration file and simplifies the deployment into one function call.</p> <p>Configuration file location (supports any location in git repository):</p> <pre><code>C:/dev/workspace\n    /HelloWorld.Notebook\n        ...\n    /GoodbyeWorld.Notebook\n        ...\n    /config.yml\n</code></pre> <p>Basic example of configuration-based deployment:</p> <pre><code>from fabric_cicd import deploy_with_config\n\n# Deploy using a config file\ndeploy_with_config(\n    config_file_path=\"C:/dev/workspace/config.yml\",\n    environment=\"dev\"\n)\n</code></pre> <p>IMPORTANT: Configuration-based deployment is currently an experimental feature and requires feature flags <code>enable_experimental_features</code> and <code>enable_config_deploy</code> to be set.</p> <p>Raise a feature request for additional capabilities or a bug report for issues.</p>"},{"location":"how_to/config_deployment/#configuration-file-setup","title":"Configuration File Setup","text":"<p>The configuration file includes several sections with configurable settings for different aspects of the deployment process.</p> <p>Note: Configuration values can be specified in two ways: as a single value (applied to any environment provided) or as an environment mapping. Both approaches can be used within the same configuration file - for example, using environment mappings for workspace IDs while keeping a single value for repository directory.</p>"},{"location":"how_to/config_deployment/#core-settings","title":"Core Settings","text":"<p>The <code>core</code> section is required as it defines the fundamental settings for the deployment, most importantly the target workspace and repository directory. Other optional settings can be configured within the <code>core</code> section, which include item types in scope and parameter.</p> <pre><code>core:\n    # Only one workspace identifier field is required\n    workspace: &lt;workspace_name&gt;\n\n    workspace_id: &lt;workspace_id&gt;\n\n    # Required - path to the directory containing Fabric items\n    repository_directory: &lt;rel_or_abs_path_of_repo_dir&gt;\n\n    # Optional - specific item types to include in deployment\n    item_types_in_scope:\n        - &lt;item_type_1&gt;\n        - &lt;item_type_2&gt;\n        - &lt;item_type..&gt;\n\n    # Optional - path to parameter file\n    parameter: &lt;rel_or_abs_path_of_param_file&gt;\n</code></pre> <p>With environment mapping:</p> <pre><code>core:\n    # Only one workspace identifier field is required\n    workspace:\n        &lt;env_1&gt;: &lt;env_1_workspace_name&gt;\n        &lt;env..&gt;: &lt;env.._workspace_name&gt;\n\n    workspace_id:\n        &lt;env_1&gt;: &lt;env_1_workspace_id&gt;\n        &lt;env..&gt;: &lt;env.._workspace_id&gt;\n\n    # Required - path to the directory containing Fabric items\n    repository_directory:\n        &lt;env_1&gt;: &lt;rel_or_abs_path_of_repo_dir_1&gt;\n        &lt;env..&gt;: &lt;rel_or_abs_path_of_repo_dir..&gt;\n\n    # Optional - specific item types to include in deployment\n    item_types_in_scope:\n        &lt;env_1&gt;:\n            - &lt;item_type_1&gt;\n            - &lt;item_type..&gt;\n        &lt;env..&gt;:\n            - &lt;item_type_1&gt;\n            - &lt;item_type..&gt;\n\n    # Optional - path to parameter file\n    parameter:\n        &lt;env_1&gt;: &lt;rel_or_abs_path_of_param_file_1&gt;\n        &lt;env..&gt;: &lt;rel_or_abs_path_of_param_file..&gt;\n</code></pre> <p>Required Fields:</p> <ul> <li>Workspace Identifier:<ul> <li>Workspace ID takes precedence over workspace name when both are provided.</li> <li><code>workspace_id</code> must be a valid string GUID.</li> </ul> </li> <li>Repository Directory Path:<ul> <li>Supports relative or absolute path.</li> <li>Relative path must be relative to the <code>config.yml</code> file location.</li> </ul> </li> </ul> <p>Optional Fields:</p> <ul> <li>Item Types in Scope:<ul> <li>If <code>item_types_in_scope</code> is not specified, all item types will be included by default.</li> <li>Item types must be provided as a list, use <code>-</code> or <code>[]</code> notation.</li> <li>Only accepts supported item types.</li> </ul> </li> <li>Parameter Path:<ul> <li>Supports relative or absolute path.</li> <li>Relative path must be relative to the <code>config.yml</code> file location.</li> </ul> </li> </ul>"},{"location":"how_to/config_deployment/#publish-settings","title":"Publish Settings","text":"<p><code>publish</code> is optional and can be used to control item publishing behavior. It includes various optional settings to enable/disable publishing operations or selectively publish items.</p> <pre><code>publish:\n    # Optional - pattern to exclude items from publishing\n    exclude_regex: &lt;regex_pattern_string&gt;\n\n    # Optional - specific items to publish (requires feature flags)\n    items_to_include:\n        - &lt;item_name.item_type_1&gt;\n        - &lt;item_name.item_type..&gt;\n\n    # Optional - control publishing by environment\n    skip: &lt;bool_value&gt;\n</code></pre> <p>With environment mapping:</p> <pre><code>publish:\n    # Optional - pattern to exclude items from publishing\n    exclude_regex:\n        &lt;env_1&gt;: &lt;regex_pattern_string_1&gt;\n        &lt;env..&gt;: &lt;regex_pattern_string..&gt;\n\n    # Optional - specific items to publish (requires feature flags)\n    items_to_include:\n        - &lt;item_name.item_type_1&gt;\n        - &lt;item_name.item_type..&gt;\n\n    # Optional - control publishing by environment\n    skip:\n        &lt;env_1&gt;: &lt;bool_value&gt;\n        &lt;env..&gt;: &lt;bool_value&gt;\n</code></pre>"},{"location":"how_to/config_deployment/#unpublish-settings","title":"Unpublish Settings","text":"<p><code>unpublish</code> is optional and can be used to control item unpublishing behavior. It includes various optional settings to enable/disable unpublishing or selectively unpublish items.</p> <pre><code>unpublish:\n    # Optional - pattern to exclude items from unpublishing\n    exclude_regex: &lt;regex_pattern_string&gt;\n\n    # Optional - specific items to unpublish (requires feature flags)\n    items_to_include:\n        - &lt;item_name.item_type_1&gt;\n        - &lt;item_name.item_type..&gt;\n\n    # Optional - control unpublishing by environment\n    skip: &lt;bool_value&gt;\n</code></pre> <p>With environment mapping:</p> <pre><code>unpublish:\n    # Optional - pattern to exclude items from unpublishing\n    exclude_regex:\n        &lt;env_1&gt;: &lt;regex_pattern_string_1&gt;\n        &lt;env..&gt;: &lt;regex_pattern_string..&gt;\n\n    # Optional - specific items to unpublish (requires feature flags)\n    items_to_include:\n        &lt;env_1&gt;:\n            - &lt;item_name.item_type_1&gt;\n        &lt;env..&gt;:\n            - &lt;item_name.item_type..&gt;\n\n    # Optional - control unpublishing by environment\n    skip:\n        &lt;env_1&gt;: &lt;bool_value&gt;\n        &lt;env..&gt;: &lt;bool_value&gt;\n</code></pre> <p>Warning: While selective deployment is supported in <code>fabric-cicd</code> it is not recommended due to potential issues with dependency management.</p>"},{"location":"how_to/config_deployment/#features-setting","title":"Features Setting","text":"<p><code>features</code> is optional and can be used to set a list of specific feature flags.</p> <pre><code>features:\n    - &lt;feature_flag_1&gt;\n    - &lt;feature_flag..&gt;\n</code></pre> <p>With environment mapping:</p> <pre><code>features:\n    &lt;env_1&gt;:\n        - &lt;feature_flag_1&gt;\n    &lt;env..&gt;:\n        - &lt;feature_flag..&gt;\n</code></pre>"},{"location":"how_to/config_deployment/#constants-setting","title":"Constants Setting","text":"<p><code>constants</code> is optional and can be used to override supported library constants.</p> <pre><code>constants:\n    CONSTANT_NAME: &lt;constant_value&gt;\n</code></pre> <p>With environment mapping:</p> <pre><code>constants:\n    CONSTANT_NAME:\n        &lt;env_1&gt;: &lt;constant_value_1&gt;\n        &lt;env..&gt;: &lt;constant_value..&gt;\n</code></pre>"},{"location":"how_to/config_deployment/#sample-configuration-file","title":"Sample Configuration File","text":"<pre><code>core:\n    workspace:\n        dev: \"Fabric-Dev-Engineering\"\n        test: \"Fabric-Test-Engineering\"\n        prod: \"Fabric-Prod-Engineering\"\n\n    workspace_id:\n        dev: \"8b6e2c7a-4c1f-4e3a-9b2e-7d8f2e1a6c3b\"\n        test: \"2f4b9e8d-1a7c-4d3e-b8e2-5c9f7a2d4e1b\"\n        prod: \"7c3e1f8b-2d4a-4b9e-8f2c-1a6c3b7d8e2f\"\n\n    repository_directory: \".\" # relative path\n\n    item_types_in_scope:\n        - Notebook\n        - DataPipeline\n        - Environment\n        - Lakehouse\n\n    parameter: \"parameter.yml\" # relative path\n\npublish:\n    # Don't publish items matching this pattern\n    exclude_regex: \"^DONT_DEPLOY.*\"\n\n    items_to_include:\n        - \"Hello World.Notebook\"\n        - \"Run Hello World.DataPipeline\"\n\n    skip:\n        dev: true\n        test: false\n        prod: false\n\nunpublish:\n    # Don't unpublish items matching this pattern\n    exclude_regex: \"^DEBUG.*\"\n\n    skip:\n        dev: false\n        test: false\n        prod: true\n\nfeatures:\n    - enable_shortcut_publish\n    - enable_experimental_features\n    - enable_items_to_include\n\nconstants:\n    DEFAULT_API_ROOT_URL: \"https://api.fabric.microsoft.com\"\n</code></pre>"},{"location":"how_to/config_deployment/#configuration-file-deployment","title":"Configuration File Deployment","text":""},{"location":"how_to/config_deployment/#basic-usage","title":"Basic Usage","text":"<pre><code>from fabric_cicd import deploy_with_config\n\n# Deploy using a config file\ndeploy_with_config(\n    config_file_path=\"path/to/config.yml\", # required\n    environment=\"dev\" # optional (recommended)\n)\n</code></pre>"},{"location":"how_to/config_deployment/#custom-authentication","title":"Custom Authentication","text":"<pre><code>from fabric_cicd import deploy_with_config\nfrom azure.identity import ClientSecretCredential\n\n# Create a credential\ncredential = ClientSecretCredential(\n    tenant_id=\"your-tenant-id\",\n    client_id=\"your-client-id\",\n    client_secret=\"your-client-secret\"\n)\n\n# Deploy with custom credential\ndeploy_with_config(\n    config_file_path=\"path/to/config.yml\",\n    environment=\"prod\",\n    token_credential=credential\n)\n</code></pre>"},{"location":"how_to/config_deployment/#configuration-override","title":"Configuration Override","text":"<p>The <code>config_override</code> parameter in <code>deploy_with_config()</code> allows you to dynamically modify configuration values at runtime without changing the base configuration file. This is particularly useful for debugging or making temporary deployment adjustments.</p> <pre><code>from fabric_cicd import deploy_with_config\n\nconfig_override_dict = {\n    \"core\": {\n        \"item_types_in_scope\": [\"Notebook\", \"DataPipeline\"]\n    },\n    \"publish\": {\n        \"skip\": {\n            \"dev\": False\n        }\n    }\n}\n\n# Deploy with configuration override\ndeploy_with_config(\n    config_file_path=\"path/to/config.yml\",\n    environment=\"dev\",\n    config_override=config_override_dict\n)\n</code></pre> <p>Important Considerations:</p> <ul> <li>Caution: Exercise caution when overriding configuration values for production environments.</li> <li>Support: Configuration overrides are supported for all sections and settings in the configuration file.</li> <li>Rules:<ul> <li>Existing values can be overridden for any field in the configuration.</li> <li>New values can only be added for optional fields that aren't present in the original configuration.</li> <li>Required fields must exist in the original configuration in order to override.</li> </ul> </li> </ul>"},{"location":"how_to/config_deployment/#troubleshooting-guide","title":"Troubleshooting Guide","text":"<p>The configuration file undergoes validation prior to reaching the deployment phase. Please note some common issues that may occur:</p> <p>1. File Not Found: Ensure the configuration file path is correct and accessible (must be an absolute path).</p> <p>2. Invalid YAML: Check YAML syntax for errors (indentation, missing quotes, etc.).</p> <p>3. Missing Required Fields: Ensure <code>core</code> section is present and contains the required fields (workspace identifier, repository directory path).</p> <p>4. Path Resolution Errors: Relative paths are resolved relative to the <code>config.yml</code> file location. Check path inputs are valid and accessible.</p> <p>5. Environment Not Found: The <code>environment</code> parameter must match one of the environment keys (like \"dev\", \"test\", \"prod\") used in the configuration mappings.</p>"},{"location":"how_to/getting_started/","title":"Getting Started","text":""},{"location":"how_to/getting_started/#installation","title":"Installation","text":"<p>To install fabric-cicd, run:</p> <pre><code>pip install fabric-cicd\n</code></pre>"},{"location":"how_to/getting_started/#authentication","title":"Authentication","text":"<ul> <li>You can optionally provide your own credential object that aligns with the <code>TokenCredential</code> class. For more details, see the TokenCredential documentation.</li> <li> <p>If you do not provide a <code>token_credential</code> parameter, the library will use the Azure SDK's <code>DefaultAzureCredential</code> for authentication.</p> <ul> <li>Refer to the Azure SDK documentation for the order in which credential types are attempted.</li> <li> <p>For local development with a User Principal Name (UPN), install either the Azure CLI or the Az.Accounts PowerShell module.</p> </li> <li> <p>Note: When no credential is provided, the <code>DefaultAzureCredential</code> may select an unexpected identity. For example, if you log in to the Azure CLI with a Service Principal Name (SPN) but log in to Az.Accounts with a UPN, the <code>DefaultAzureCredential</code> will prioritize the CLI authentication.</p> </li> </ul> </li> </ul>"},{"location":"how_to/getting_started/#directory-structure","title":"Directory Structure","text":"<p>This library deploys from a directory containing files and directories committed via the Fabric Source Control UI. Ensure the <code>repository_directory</code> includes only these committed items, with the exception of the <code>parameter.yml</code> file.</p> <pre><code>/&lt;your-directory&gt;\n    /&lt;item-name&gt;.&lt;item-type&gt;\n        ...\n    /&lt;item-name&gt;.&lt;item-type&gt;\n        ...\n    /&lt;workspace-subfolder&gt;\n        /&lt;item-name&gt;.&lt;item-type&gt;\n            ...\n        /&lt;item-name&gt;.&lt;item-type&gt;\n            ...\n    /parameter.yml\n</code></pre>"},{"location":"how_to/getting_started/#git-flow","title":"GIT Flow","text":"<p>The flow pictured below is the hero scenario for this library and is the recommendation if you're just starting out.</p> <ul> <li><code>Deployed</code> branches are not connected to workspaces via GIT Sync</li> <li><code>Feature</code> branches are connected to workspaces via GIT Sync</li> <li><code>Deployed</code> workspaces are only updated through script-based deployments, such as through the fabric-cicd library</li> <li><code>Feature</code> branches are created from the default branch, merged back into the default <code>Deployed</code> branch, and cherry picked into the upper <code>Deployed</code> branches</li> <li>Each deployment is a full deployment and does not consider commit diffs</li> </ul> <p></p>"},{"location":"how_to/item_types/","title":"Item Types","text":""},{"location":"how_to/item_types/#activators","title":"Activators","text":"<ul> <li>Parameterization:<ul> <li>The <code>find_replace</code> section in the <code>parameter.yml</code> file is not applied.</li> </ul> </li> <li>Initial deployment may not reflect streaming data immediately.</li> <li>Reflex is the item name in source control. Source control may not support all activators/reflexes, as not all sources are compatible.</li> </ul>"},{"location":"how_to/item_types/#api-for-graphql","title":"API for GraphQL","text":"<ul> <li>Parameterization:<ul> <li>The source will always point to the source in the original workspace unless parameterized in the <code>find_replace</code> section of the <code>parameter.yml</code> file.</li> <li>It is recommended to use the supported variables in <code>find_replace</code> for dynamic replacement of the source workspace and item IDs.</li> <li>If you are using a connection and expect it to change between different environments, then it needs to be parameterized in the <code>parameter.yml</code> file.</li> </ul> </li> <li>When using the Saved Credential method to connect to data sources, developers must have access to the Saved Credential information in order to successfully deploy GraphQL item.</li> <li>Changes made to the original API query are not source controlled. You will need to manually update the query in the GraphQL item's query editor within the target workspace.</li> <li>Only user authentication is currently supported for GraphQL items that source data from the SQL Analytics Endpoint.</li> </ul>"},{"location":"how_to/item_types/#copy-jobs","title":"Copy Jobs","text":"<ul> <li>Parameterization:<ul> <li>Connections will always point to the original data source unless parameterized in the <code>find_replace</code> section of the <code>parameter.yml</code> file.</li> </ul> </li> <li>Initial deployment requires manual configuration of the connection after deployment.</li> </ul>"},{"location":"how_to/item_types/#dataflows","title":"Dataflows","text":"<ul> <li>Parameterization:<ul> <li>Source/destination items (e.g., Dataflow, Lakehouse, Warehouse) will always reference the original item unless parameterized in the <code>find_replace</code> section of the <code>parameter.yml</code> file.</li> <li>The recommended approach for re-pointing source/destination items that exist in the same workspace as the Dataflow is to use <code>replace_value</code> variables in the <code>find_replace</code> parameter along with a <code>find_value</code> regex (literal string works too). For more guidance, see Parameterization -&gt; Dataflows.</li> <li>Important for Dataflows that reference another Dataflow in the same workspace, ONLY parameterize the referenced dataflowId in the <code>mashup.pq</code> file (workspaceId re-pointing is handled automatically in this approach) using the following <code>replace_value</code>: <code>$items.Dataflow.&lt;The Source Dataflow Name&gt;.id</code>. This ensures proper dependency resolution and ordered publishing.</li> </ul> </li> <li>Initial deployment will require a manual publish and refresh of the dataflow. After re-pointing dataflows during deployment, temporary errors may appear but should resolve after refreshing and allowing time for processing (especially when a dataflow sources from another dataflow in the target workspace).</li> <li><code>fabric ci-cd</code> automatically manages ordered deployment of dataflows that source from other dataflows in the same workspace.</li> <li>Connections are not source controlled and require manual creation.</li> <li>If you use connections that differ between environments, parameterize them in the <code>parameter.yml</code> file.</li> </ul>"},{"location":"how_to/item_types/#data-pipelines","title":"Data Pipelines","text":"<ul> <li>Parameterization:<ul> <li>Activities connected to items that exist in a different workspace will always point to the original item unless parameterized in the <code>find_replace</code> section of the <code>parameter.yml</code> file.</li> <li>Activities connected to items within the same workspace are automatically re-pointed to the new item in the target workspace. Note: Activities referencing items that don't use logical ID and default workspace ID (such as Refresh Dataflow and Refresh Semantic Model) require parameterization.</li> </ul> </li> <li>Connections are not source controlled and must be created manually.</li> <li>If you are using connections and expect them to change between different environments, then those need to be parameterized in the <code>parameter.yml</code> file.</li> <li>The executing identity of the deployment must have access to the connections, or the deployment will fail.</li> </ul>"},{"location":"how_to/item_types/#environments","title":"Environments","text":"<ul> <li>Parameterization:<ul> <li>Environments attached to custom spark pools attach to the default starter pool unless parameterized in the <code>spark_pools</code> section of the <code>parameter.yml</code> file.</li> <li>The <code>find_replace</code> section in the <code>parameter.yml</code> file is not applied to Environments.</li> </ul> </li> <li>Resources are not source controlled and will not be deployed.</li> <li>Environments with libraries will have high initial publish times (sometimes 20+ minutes).</li> </ul>"},{"location":"how_to/item_types/#eventhouses","title":"Eventhouses","text":"<ul> <li>Parameterization:<ul> <li>The <code>find_replace</code> section in the <code>parameter.yml</code> file is not applied.</li> </ul> </li> <li>The <code>exclude_path</code> variable is required when deploying an Eventhouse that is attached to a KQL Database (common scenario).</li> <li>There may be significant differences in the streaming data between the source eventhouse and the deployed eventhouse.</li> <li>Unpublish is disabled by default, enable with feature flag <code>enable_eventhouse_unpublish</code>.</li> </ul>"},{"location":"how_to/item_types/#eventstreams","title":"Eventstreams","text":"<ul> <li>Parameterization:<ul> <li>Destinations connected to items that exist in a different workspace will always point to the original item unless parameterized in the <code>find_replace</code> section of the <code>parameter.yml</code> file.</li> <li>Destinations connected to items within the same workspace are re-pointed to the new item in the target workspace.</li> </ul> </li> <li>Initial deployment requires waiting for the table to populate in the destination lakehouse if a lakehouse destination is present in the eventstream.</li> </ul>"},{"location":"how_to/item_types/#kql-databases","title":"KQL Databases","text":"<ul> <li>Parameterization:<ul> <li>The <code>find_replace</code> section in the <code>parameter.yml</code> file is not applied.</li> </ul> </li> <li>In Fabric, a KQL database is not a standalone item. However, during deployment, it is treated as such. Its source control files are located within a <code>.children</code> folder under the directory of the attached eventhouse.</li> <li>Data in KQL database tables is not source controlled and may not consistently appear in the database UI after deployment. Some tables may be empty post-deployment.</li> </ul>"},{"location":"how_to/item_types/#kql-querysets","title":"KQL Querysets","text":"<ul> <li>Parameterization:<ul> <li>KQL querysets attached to KQL databases always point to the original KQL database unless parameterized in the <code>find_replace</code> section of the <code>parameter.yml</code> file.</li> </ul> </li> <li>The cluster/query URI of the KQL database must be present in the KQL queryset JSON for rebinding. If the KQL queryset is attached to a KQL database within the same workspace, the cluster URI value is empty and needs to be re-added. <code>fabric ci-cd</code> handles this automatically.</li> <li>KQL querysets can still exist after the KQL database source has been deleted. However, errors will reflect in the KQL queryset.</li> </ul>"},{"location":"how_to/item_types/#lakehouses","title":"Lakehouses","text":"<ul> <li>Parameterization:<ul> <li>The <code>find_replace</code> section in the <code>parameter.yml</code> file is not applied.</li> </ul> </li> <li>Shortcut publish is disabled by default (for now), enable with feature flag <code>enable_shortcut_publish</code>.</li> <li>Schemas are not deployed unless the schema has a shortcut present.</li> <li>Unpublish is disabled by default, enable with feature flag <code>enable_lakehouse_unpublish</code>.</li> </ul>"},{"location":"how_to/item_types/#mirrored-databases","title":"Mirrored Databases","text":"<ul> <li>Parameterization:<ul> <li>Connections will always point to the original source database unless parameterized in the <code>find_replace</code> section of the <code>parameter.yml</code> file.</li> </ul> </li> <li>Initial deployment for Azure SQL Database or Azure SQL Managed Instance requires manual granting of System Assigned Managed Identity (SAMI) Read and Write permission to the mirrored database for replication to be successful after deployment. ref -&gt; (Prerequisites)</li> <li>Unpublish - a warning is shown for any default Semantic Models created by the Mirror Database. This is a current limitation of the Fabric API and can be ignored.</li> </ul>"},{"location":"how_to/item_types/#notebooks","title":"Notebooks","text":"<ul> <li>Parameterization:<ul> <li>Notebooks attached to lakehouses always point to the original lakehouse unless parameterized in the <code>find_replace</code> section of the <code>parameter.yml</code> file.</li> </ul> </li> <li>Resources are not source controlled and will not be deployed.</li> </ul>"},{"location":"how_to/item_types/#real-time-dashboard","title":"Real-Time Dashboard","text":"<ul> <li>Parameterization:<ul> <li>Real-Time Dashboard attached to KQL databases always point to the original KQL database unless parameterized in the <code>find_replace</code> section of the <code>parameter.yml</code> file.</li> </ul> </li> <li>The cluster/query URI of the KQL database(s) used in the dashboard must be present in the Real-Time Dashboard JSON for rebinding. If the Real-Time Dashboard is attached to a KQL database within the same workspace, the cluster URI value is empty and needs to be re-added. <code>fabric ci-cd</code> handles this automatically.</li> </ul>"},{"location":"how_to/item_types/#reports","title":"Reports","text":"<ul> <li>Parameterization:<ul> <li>Reports connected to Semantic Models outside of the same workspace always point to the original Semantic Model unless parameterized in the <code>find_replace</code> section of the <code>parameter.yml</code> file.</li> <li>Reports connected to Semantic Models within the same workspace are re-pointed to the new item in the target workspace.</li> </ul> </li> </ul>"},{"location":"how_to/item_types/#semantic-models","title":"Semantic Models","text":"<ul> <li>Parameterization:<ul> <li>Semantic Models connected to sources outside of the same workspace always point to the original item unless parameterized in the <code>find_replace</code> section of the <code>parameter.yml</code> file.</li> <li>Semantic Models connected to sources within the same workspace may or may not be re-pointed; it is best to test this before taking a dependency. Use the <code>find_replace</code> section of the <code>parameter.yml</code> file as needed.</li> </ul> </li> <li>Initial deployment requires manual configuration of the connection after deployment.</li> </ul>"},{"location":"how_to/item_types/#sql-databases","title":"SQL Databases","text":"<ul> <li>Parameterization:<ul> <li>The <code>find_replace</code> section in the <code>parameter.yml</code> file is not applied.</li> </ul> </li> <li>SQL Database content is not deployed. Only the item shell is deployed. The SQL database schema (DDL) must be deployed separately using a DACPAC or other tools such as dbt.</li> <li>Unpublish is disabled by default, enable with feature flag <code>enable_sqldatabase_unpublish</code>.</li> </ul>"},{"location":"how_to/item_types/#variable-libraries","title":"Variable Libraries","text":"<ul> <li>Parameterization:<ul> <li>The active value set of the variable library is defined by the <code>environment</code> field passed into the <code>FabricWorkspace</code> object. If no <code>environment</code> is specified, the active Value Set will not be changed.</li> </ul> </li> <li>Changing Value Sets:<ul> <li>Variable Libraries do not support programmatically changing the name of value set which is active</li> <li>After the initial deployment, if an active set is renamed, or removed, the deployment will fail</li> <li>Manual intervention will be required to make the necessary changes in the Fabric UI and then restart the deployment</li> </ul> </li> </ul>"},{"location":"how_to/item_types/#warehouses","title":"Warehouses","text":"<ul> <li>Parameterization:<ul> <li>The <code>find_replace</code> section in the <code>parameter.yml</code> file is not applied.</li> </ul> </li> <li>Warehouse content is not deployed. Only the item shell is deployed. Warehouse DDL must be deployed separately using a DACPAC or other tools such as dbt.</li> <li>Case insensitive collation is supported custom collation must be manually edited in the <code>.platform</code> file creation payload. See How to: Create a warehouse with case-insensitive (CI) collation for more details.</li> <li>Unpublish is disabled by default, enable with feature flag <code>enable_warehouse_unpublish</code>.</li> </ul>"},{"location":"how_to/optional_feature/","title":"Optional Features","text":"<p>fabric-cicd has an expected default flow; however, there will always be cases where overriding default behavior is required.</p>"},{"location":"how_to/optional_feature/#feature-flags","title":"Feature Flags","text":"<p>For scenarios that aren't supported by default, fabric-cicd offers <code>feature-flags</code>. Below is an exhaustive list of currently supported features.</p> Flag Name Description Experimental <code>enable_lakehouse_unpublish</code> Set to enable the deletion of Lakehouses <code>enable_warehouse_unpublish</code> Set to enable the deletion of Warehouses <code>enable_sqldatabase_unpublish</code> Set to enable the deletion of SQL Databases <code>enable_eventhouse_unpublish</code> Set to enable the deletion of Eventhouses <code>disable_print_identity</code> Set to disable printing the executing identity name <code>enable_shortcut_publish</code> Set to enable deploying shortcuts with the lakehouse <code>enable_environment_variable_replacement</code> Set to enable the use of pipeline variables <code>disable_workspace_folder_publish</code> Set to disable deploying workspace sub folders <code>enable_experimental_features</code> Set to enable experimental features, such as selective deployments <code>enable_items_to_include</code> Set to enable selective publishing/unpublishing of items \u2611\ufe0f <code>enable_config_deploy</code> Set to enable config file-based deployment \u2611\ufe0f <p>Example</p> <pre><code>from fabric_cicd import append_feature_flag\nappend_feature_flag(\"enable_lakehouse_unpublish\")\nappend_feature_flag(\"enable_warehouse_unpublish\")\nappend_feature_flag(\"disable_print_identity\")\nappend_feature_flag(\"enable_environment_variable_replacement\")\n</code></pre> <p>Experimental Features</p> <p>To use experimental features, such as selective deployments (e.g., specifying a list of items to publish/unpublish), you must enable both the <code>enable_experimental_features</code> flag and the flag specific to the feature, such as <code>enable_items_to_include</code>.</p>"},{"location":"how_to/optional_feature/#debugging","title":"Debugging","text":"<p>If an error arises, or you want to have full transparency to all calls being made outside the library, enable debugging. Enabling debugging will write all API calls to the terminal and to the <code>fabric-cicd.log</code>.</p> <pre><code>from fabric_cicd import change_log_level\nchange_log_level(\"DEBUG\")\n</code></pre>"},{"location":"how_to/parameterization/","title":"Parameterization","text":""},{"location":"how_to/parameterization/#overview","title":"Overview","text":"<p>To handle environment-specific values committed to git, use a <code>parameter.yml</code> file. This file supports programmatically changing values based on the <code>environment</code> field passed into the <code>FabricWorkspace</code> object. If the environment value is not found in the <code>parameter.yml</code> file, any dependent replacements will be skipped. This file should sit in the root of the <code>repository_directory</code> folder specified in the FabricWorkspace object.</p> <p>Example of parameter.yml location based on provided repository directory:</p> <pre><code>from fabric_cicd import FabricWorkspace\nworkspace = FabricWorkspace(\n    workspace_id=\"your-workspace-id\",\n    repository_directory=\"C:/dev/workspace\",\n    item_type_in_scope=[\"Notebook\"]\n)\n</code></pre> <pre><code>C:/dev/workspace\n    /HelloWorld.Notebook\n        ...\n    /GoodbyeWorld.Notebook\n        ...\n    /parameter.yml\n</code></pre> <p>Example of parameter.yml file content:</p> <pre><code>find_replace:\n    - find_value: \"your-dev-lakehouse-id\"\n      replace_value:\n          PPE: \"ppe-lakehouse-id\"\n          PROD: \"prod-lakehouse-id\"\n\nkey_value_replace:\n    - find_key: $.variables[?(@.name==\"Environment\")].value\n      replace_value:\n          PPE: \"PPE\"\n          PROD: \"PROD\"\n\nspark_pool:\n    - instance_pool_id: \"your-dev-pool-instance-id\"\n      replace_value:\n          PPE:\n              type: \"Capacity\"\n              name: \"PPE-Pool-name\"\n          PROD:\n              type: \"Capacity\"\n              name: \"PROD-Pool-name\"\n</code></pre> <p>Raise a feature request for additional parameterization capabilities.</p>"},{"location":"how_to/parameterization/#parameter-inputs","title":"Parameter Inputs","text":""},{"location":"how_to/parameterization/#find_replace","title":"<code>find_replace</code>","text":"<p>For generic find-and-replace operations. This will replace every instance of a specified string in every file. Specify the <code>find_value</code> and the <code>replace_value</code> for each environment (e.g., PPE, PROD). Optional fields, including <code>item_type</code>, <code>item_name</code>, and <code>file_path</code>, can be used as file filters for more fine-grained control over where the replacement occurs. The <code>is_regex</code> field can be added and set to <code>\"true\"</code> to enable regex pattern matching for the <code>find_value</code>.</p> <p>Note: A common use case for this function is to replace values in text based file types like notebooks.</p> <pre><code>find_replace:\n    # Required fields: value must be a string\n    - find_value: &lt;find-this-value&gt;\n      replace_value:\n          &lt;environment-1-key&gt;: &lt;replace-with-this-value&gt;\n          &lt;environment-2-key&gt;: &lt;replace-with-this-value&gt;\n      # Optional fields\n      # Set to \"true\" to treat find_value as a regex pattern\n      is_regex: \"&lt;true|True&gt;\"\n      # Filter values must be a string or array of strings\n      item_type: &lt;item-type-filter-value&gt;\n      item_name: &lt;item-name-filter-value&gt;\n      file_path: &lt;file-path-filter-value&gt;\n</code></pre>"},{"location":"how_to/parameterization/#key_value_replace","title":"<code>key_value_replace</code>","text":"<p>Provides the ability to perform key based replacement operations in JSON and YAML files. This will look for a specific key using a valid JSONPath expression and replace every found instance in every file. Specify the <code>find_value</code> and the <code>replace_value</code> for each environment (e.g., PPE, PROD). Optional fields, including <code>item_type</code>, <code>item_name</code>, and <code>file_path</code>, can be used as file filters for more fine-grained control over where the replacement occurs. Refer to https://jsonpath.com/ for a simple to use JSONPath evaluator.</p> <p>Note: A common use case for this function is to replace values in key/value file types like Pipelines, Platform files, Schedules files, etc. The function automatically detects and processes any file containing valid JSON content, regardless of file extension (e.g., <code>.schedules</code>, <code>.platform</code> files).</p> <pre><code>key_value_replace:\n    # Required fields: key must be JSONPath\n    - find_key: &lt;find-this-key&gt;\n      replace_value:\n          &lt;environment-1-key&gt;: &lt;replace-with-this-value&gt;\n          &lt;environment-2-key&gt;: &lt;replace-with-this-value&gt;\n      # Optional fields: value must be a string or array of strings\n      item_type: &lt;item-type-filter-value&gt;\n      item_name: &lt;item-name-filter-value&gt;\n      file_path: &lt;file-path-filter-value&gt;\n</code></pre>"},{"location":"how_to/parameterization/#spark_pool","title":"<code>spark_pool</code>","text":"<p>Environments attached to custom spark pools need to be parameterized because the <code>instance_pool_id</code> in the <code>Sparkcompute.yml</code> file isn't supported in the create/update environment APIs. Provide the <code>instance_pool_id</code> value, and the pool <code>type</code> and <code>name</code> values as the <code>replace_value</code> for each environment (e.g., PPE, PROD). An optional field, <code>item_name</code>, can be used to filter the specific environment item where the replacement will occur.</p> <pre><code>spark_pool:\n    # Required fields: value must be a string\n    - instance_pool_id: &lt;instance-pool-id-value&gt;\n      replace_value:\n          &lt;environment-1-key&gt;:\n              type: &lt;Capacity-or-Workspace&gt;\n              name: &lt;pool-name&gt;\n          &lt;environment-2-key&gt;:\n              type: &lt;Capacity-or-Workspace&gt;\n              name: &lt;pool-name&gt;\n      # Optional field: value must be a string or array of strings\n      item_name: &lt;item-name-filter-value&gt;\n</code></pre>"},{"location":"how_to/parameterization/#advanced-find-and-replace","title":"Advanced Find and Replace","text":""},{"location":"how_to/parameterization/#find_value-regex","title":"<code>find_value</code> Regex","text":"<p>In the <code>find_replace</code> parameter, the <code>find_value</code> can be set to a regex pattern instead of a literal string to find a value in the files to replace. When a match is found, the <code>find_value</code> is assigned to the matched string and can be used to replace all occurrences of that value in the file.</p> <ul> <li>How to use this feature:<ul> <li>Set the <code>find_value</code> to a valid regex pattern wrapped in quotes.</li> <li>Include the optional field <code>is_regex</code> and set it to the value <code>\"true\"</code>, see more details.</li> </ul> </li> <li>Important:<ul> <li>The user is solely responsible for providing a valid and correctly matching regex pattern. If the pattern is invalid (i.e., it cannot be compiled) or fails to match any content in the target files, deployment will fail.</li> <li>A valid regex pattern requires the following:<ul> <li>Ensure that all special characters in the regex pattern are properly escaped.</li> <li>The exact value intended to be replaced must be enclosed in parentheses <code>( )</code>.</li> <li>The parentheses creates a capture group 1, which must always be used as the replacement target. Capture group 1 should isolate values like a GUID, SQL connection string, etc.</li> <li>Include the surrounding context in the pattern, such as property/field names, quotes, etc. to ensure it matches the correct value and not a value with a similar format elsewhere in the file.</li> </ul> </li> </ul> </li> <li>Example:<ul> <li>Use a regex <code>find_value</code> to match a lakehouse ID inside a Notebook file. Note: avoid using a pattern that ONLY matches the GUID format as doing so would risk replacing any matching GUID in the file, not just the intended one. Include the surrounding context in your pattern\u2014such as <code># META \"default_lakehouse\": \"123456\"</code>\u2014and capture only the <code>123456</code> GUID in group 1. This ensures that only the correct, context-specific GUID is replaced.</li> </ul> </li> </ul> <pre><code>find_replace:\n    # A valid regex pattern to match the default_lakehouse ID\n    - find_value: \\#\\s*META\\s+\"default_lakehouse\":\\s*\"([0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12})\"\n      replace_value:\n          PPE: \"e8a7f3c6-9b2d-4f5e-a1b0-7c98d4e6a5f3\" # PPE Lakehouse GUID\n          PROD: \"12c45d67-89ab-4cde-f012-3456789abcde\" # PROD Lakehouse GUID\n      # Optional field: Set to \"true\" to treat find_value as a regex pattern\n      is_regex: \"true\" # \"&lt;true|True&gt;\"\n      item_type: \"Notebook\" # filter on notebook files\n      item_name: [\"Hello World\", \"Goodbye World\"] # filter on specific notebook files\n</code></pre>"},{"location":"how_to/parameterization/#dynamic-replacement","title":"Dynamic Replacement","text":"<p>The <code>replace_value</code> field in the <code>find_replace</code> parameter supports fabric-cicd defined variables that reference workspace or deployed item metadata:</p> <ul> <li>Dynamic workspace/item metadata replacement ONLY works for referenced items that exist in the <code>repository_directory</code>.</li> <li>Dynamic replacement works in tandem with <code>find_value</code> as either a regex or a literal string.</li> <li>The <code>replace_value</code> can contain a mix of input values within the same parameter input, e.g. <code>PPE</code> is set to a static string and <code>PROD</code> is set to a variable.</li> <li>Supported variables:<ul> <li>Workspace ID variable: <code>$workspace.id</code>, replaces a workspace ID with the workspace ID of the target environment.</li> <li>Item attribute variable: <code>$items.&lt;item_type&gt;.&lt;item_name&gt;.&lt;attribute&gt;</code>, replaces the item's attribute value with the corresponding attribute value of the item in the deployed/target workspace.<ul> <li>Supported attributes: <code>id</code> (item ID of the deployed item), <code>sqlendpoint</code> (sql connection string of the deployed item, only applicable to lakehouse and warehouse items), and <code>queryserviceuri</code> (query uri of the deployed item, only applicable to eventhouse item). Attributes should be lowercase.</li> <li>Item type and name are case-sensitive.</li> <li>Item type must be valid and in scope.</li> <li>Item name must be an exact match (include spaces, if present).</li> <li>Example: set <code>$items.Notebook.Hello World.id</code> to get the item ID of the <code>\"Hello World\"</code> Notebook in the target workspace.</li> </ul> </li> </ul> </li> <li>Important: Deployment will fail in the following cases:<ul> <li>Incorrect variable syntax used, e.g., <code>$item.Notebook.Hello World.id</code> instead of <code>$items.Notebook.Hello World.id</code>.</li> <li>The specified item type or name is invalid or does NOT exist in the deployed workspace, e.g., <code>$items.Notebook.HelloWorld.id</code> or <code>$items.Environment.Hello World.id</code>.</li> <li>An invalid attribute name is provided, e.g., <code>$items.Notebook.Hello World.guid</code> instead of <code>$items.Notebook.Hello World.id</code>.</li> <li>The attribute value does NOT exist, e.g., <code>$items.Notebook.Hello World.sqlendpoint</code> (Notebook items don't have a SQL Endpoint).</li> </ul> </li> <li>For example use-cases, see the Notebook/Dataflow Advanced <code>find_replace</code> Parameterization Case.</li> </ul> <pre><code>find_replace:\n    - find_value: \"db52be81-c2b2-4261-84fa-840c67f4bbd0\" # Lakehouse GUID\n      replace_value:\n          PPE: \"$items.Lakehouse.Sample_LH.id\" # PPE Sample_LH Lakehouse GUID\n          PROD: \"$items.Lakehouse.Sample_LH.id\" # PROD Sample_LH Lakehouse GUID\n    - find_value: \"123e4567-e89b-12d3-a456-426614174000\" # Workspace ID\n      replace_value:\n          PPE: \"$workspace.id\" # PPE workspace ID\n          PROD: \"$workspace.id\" # PROD workspace ID\n    - find_value: \"serverconnectionstringexample.com\" # SQL endpoint connection string\n      replace_value:\n          PPE: \"$items.Lakehouse.Sample_LH.sqlendpoint\" # PPE Sample_LH Lakehouse sql endpoint\n          PROD: \"$items.Lakehouse.Sample_LH.sqlendpoint\" # PROD Sample_LH Lakehouse sql endpoint\n    - find_value: \"https://trd-a1b2c3d4e5f6g7h8i9.z4.kusto.fabric.microsoft.com\" # Eventhouse query service URI\n      replace_value:\n          PPE: \"$items.Eventhouse.Sample_EH.queryserviceuri\" # PPE Sample_EH Eventhouse query service URI\n          PROD: \"$items.Eventhouse.Sample_EH.queryserviceuri\" # PROD Sample_EH Eventhouse query service URI\n</code></pre>"},{"location":"how_to/parameterization/#environment-variable-replacement","title":"Environment Variable Replacement","text":"<p>In the <code>find_replace</code> parameter, if the <code>enable_environment_variable_replacement</code> feature flag is set, pipeline/environment variables will be used to replace the values in the <code>parameter.yml</code> file with the corresponding values from the variables dictionary. Only Environment Variable beginning with '$ENV:' will be used as replacement values. See example below:</p> <pre><code>find_replace:\n    # Lakehouse GUID\n    - find_value: \"db52be81-c2b2-4261-84fa-840c67f4bbd0\"\n      replace_value:\n          PPE: \"$ENV:ppe_lakehouse\"\n          PROD: \"$ENV:prod_lakehouse\"\n</code></pre>"},{"location":"how_to/parameterization/#file-filters","title":"File Filters","text":"<p>File filtering is supported in all parameters. This feature is optional and can be used to specify the files where replacement is intended to occur.</p> <ul> <li>Supported filters: <code>item_type</code>, <code>item_name</code>, and <code>file_path</code>, see more details.<ul> <li>Note: only <code>item_name</code> filter is supported in <code>spark_pool</code> parameter.</li> </ul> </li> <li>Expected behavior:<ul> <li>If at least one filter value does not match, the replacement will be skipped for that file.</li> <li>If none of the optional filter fields or values are provided, the value found in any repository file is subject to replacement.</li> </ul> </li> <li>Filter input:<ul> <li>Input values are case sensitive.</li> <li>Input values must be string or array (enables one or many values to filter on).<ul> <li>YAML supports array inputs using bracket ( [ ] ) or dash ( - ) notation.</li> </ul> </li> </ul> </li> </ul> <p>find_replace/key_value_replace</p> <pre><code>&lt;find_replace | key_value_replace&gt;:\n    # Required fields: value must be a string\n    - &lt;find_value | find_key&gt;: &lt;find-this-value&gt;\n      replace_value:\n          &lt;environment-1-key&gt;: &lt;replace-with-this-value&gt;\n          &lt;environment-2-key&gt;: &lt;replace-with-this-value&gt;\n      # Optional fields\n      # Filter values must be a string or array of strings\n      item_type: &lt;item-type-filter-value&gt;\n      item_name: &lt;item-name-filter-value&gt;\n      file_path: &lt;file-path-filter-value&gt;\n</code></pre> <p>spark_pool</p> <pre><code>spark_pool:\n    # Required fields: value must be a string\n    - instance_pool_id: &lt;instance-pool-id-value&gt;\n      replace_value:\n          &lt;environment-1-key&gt;:\n              type: &lt;Capacity-or-Workspace&gt;\n              name: &lt;pool-name&gt;\n          &lt;environment-2-key&gt;:\n              type: &lt;Capacity-or-Workspace&gt;\n              name: &lt;pool-name&gt;\n      # Optional field: value must be a string or array of strings\n      item_name: &lt;item-name-filter-value&gt;\n</code></pre>"},{"location":"how_to/parameterization/#_all_-environment-key-in-replace_value","title":"_ALL_ Environment Key in <code>replace_value</code>","text":"<p>The <code>_ALL_</code> environment key (case-insensitive) in <code>replace_value</code> is supported for all parameter types (<code>find_replace</code>, <code>key_value_replace</code>, <code>spark_pool</code>) and applies the replacement to any target environment. When <code>_ALL_</code> is used, it must be the only environment key in the <code>replace_value</code> dictionary. Using <code>ALL</code> without underscores will be treated as a regular environment key.</p> <p>Use case: when the same replacement value applies to all target environments (particularly valuable in dynamic replacement scenarios).</p> <pre><code>find_replace:\n    # Lakehouse GUID\n    - find_value: \"db52be81-c2b2-4261-84fa-840c67f4bbd0\"\n      replace_value:\n          # use _ALL_ or _all_ or _All_\n          _ALL_: \"$items.Lakehouse.Example_LH.id\"\n</code></pre>"},{"location":"how_to/parameterization/#optional-fields","title":"Optional Fields","text":"<p>When optional fields are omitted or left empty, only basic parameterization functionality will be available. To enable advanced features, you must add the specific optional field(s) (if applicable) and set appropriately.</p> <p>Important:</p> <ul> <li>String input values should be wrapped in quotes. Remember to escape special characters, such as \\ in <code>file_path</code> inputs.</li> <li><code>is_regex</code> and filter fields can be used in the same parameter configuration.</li> </ul>"},{"location":"how_to/parameterization/#regex-pattern-match","title":"Regex Pattern Match","text":""},{"location":"how_to/parameterization/#is_regex","title":"<code>is_regex</code>","text":"<ul> <li>Only applicable to the <code>find_replace</code> parameter.</li> <li>Include <code>is_regex</code> field when setting the <code>find_value</code> to a valid regex pattern.</li> <li>When the <code>is_regex</code> field is set to the string value <code>\"true\"</code> or <code>\"True\"</code> (case-insensitive), regex pattern matching is enabled.</li> <li>When regex pattern matching is enabled, the <code>find_value</code> is interpreted as a regex pattern rather than a literal string.</li> </ul>"},{"location":"how_to/parameterization/#supported-file-filters","title":"Supported File Filters","text":""},{"location":"how_to/parameterization/#item_type","title":"<code>item_type</code>","text":"<ul> <li>Item types must be valid and within scope of deployment.</li> <li>See valid types.</li> </ul>"},{"location":"how_to/parameterization/#item_name","title":"<code>item_name</code>","text":"<ul> <li>Item names must match the exact names of items in the <code>repository_directory</code>.</li> </ul>"},{"location":"how_to/parameterization/#file_path","title":"<code>file_path</code>","text":"<ul> <li><code>file_path</code> accepts three types of paths within the repository directory boundary:<ul> <li>Absolute paths: Full path starting from the drive root.</li> <li>Relative paths: Paths relative to the repository directory.</li> <li>Wildcard paths: Paths containing glob patterns.</li> </ul> </li> <li>When using wildcard paths:<ul> <li>Common patterns include <code>*</code> (matches any characters in a filename), <code>**</code> (matches any directory depth).</li> <li>All matched files must exist within the repository directory.</li> <li>When using wildcard patterns, verify your syntax carefully to avoid unexpected matching behavior.</li> <li>Examples: <code>**/notebook-content.py</code> matches all notebook files in the repository directory, <code>Sample Pipelines/*.json</code> matches json files in the Sample Pipelines folder in the repository directory.</li> </ul> </li> </ul>"},{"location":"how_to/parameterization/#parameter-file-validation","title":"Parameter File Validation","text":"<p>Validation of the <code>parameter.yml</code> file is a built-in feature of fabric-cicd, managed by the <code>Parameter</code> class. Validation is utilized in the following scenarios:</p> <p>Debuggability: Users can debug and validate their parameter file to ensure it meets the acceptable structure and input value criteria before running a deployment. Simply run the <code>debug_parameterization.py</code> script located in the <code>devtools</code> directory.</p> <p>Deployment: At the start of a deployment, an automated validation checks the validity of the <code>parameter.yml</code> file, if it is present. This step ensures that valid parameters are loaded, allowing deployment to run smoothly with correctly applied parameterized configurations. If the parameter file is invalid, the deployment will NOT proceed.</p>"},{"location":"how_to/parameterization/#sample-parameter-file","title":"Sample Parameter File","text":"<p>An exhaustive example of all capabilities currently supported in the <code>parameter.yml</code> file.</p> <pre><code>find_replace:\n    - find_value: \"123e4567-e89b-12d3-a456-426614174000\" # lakehouse GUID to be replaced\n      replace_value:\n          PPE: \"f47ac10b-58cc-4372-a567-0e02b2c3d479\" # PPE lakehouse GUID\n          PROD: \"9b2e5f4c-8d3a-4f1b-9c3e-2d5b6e4a7f8c\" # PROD lakehouse GUID\n      item_type: \"Notebook\" # filter on notebook files\n      item_name: [\"Hello World\", \"Goodbye World\"] # filter on specific notebook files\n\n    # enable_environment_variable_replacement feature flag to replace workspace ID\n    - find_value: \"8f5c0cec-a8ea-48cd-9da4-871dc2642f4c\" # workspace ID to be replaced\n      replace_value:\n          PPE: \"$ENV:ppe_workspace_id\" # PPE workspace ID (ENV variable)\n          PROD: \"$ENV:prod_workspace_id\" # PROD workspace ID (ENV variable)\n      file_path: # filter on notebook files with these paths\n          - \"/Hello World.Notebook/notebook-content.py\"\n          - \"\\\\Goodbye World.Notebook\\\\notebook-content.py\"\n\n    # lakehouse GUID to be replaced (using regex pattern)\n    - find_value: \\#\\s*META\\s+\"default_lakehouse\":\\s*\"([0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12})\"\n      replace_value:\n          PPE: \"$items.Lakehouse.Example_LH.id\" # PPE lakehouse GUID (dynamic)\n          PROD: \"$items.Lakehouse.Example_LH.id\" # PROD lakehouse GUID (dynamic)\n      is_regex: \"true\" # enable regex pattern matching\n      item_type: \"Notebook\" # filter on notebook files\n      item_name: [\"Hello World\", \"Goodbye World\"] # filter on specific notebook files\n\n    # lakehouse workspace ID to be replaced (using regex pattern)\n    - find_value: \\#\\s*META\\s+\"default_lakehouse_workspace_id\":\\s*\"([0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12})\"\n      replace_value:\n          _ALL_: \"$workspace.id\" # workspace ID of the target environment (dynamic)\n      is_regex: \"true\" # enable regex pattern matching\n      item_name: # filter on specific notebook files\n          - \"Hello World\"\n          - \"Goodbye World\"\n      file_path: \"**/notebook-content.py\" # filter on notebook files using wildcard paths\n\nkey_value_replace:\n    # SQL Server Connection to be replaced\n    - find_key: $.properties.activities[?(@.name==\"Load_Intake\")].typeProperties.source.datasetSettings.externalReferences.connection\n      replace_value:\n          PPE: \"6c517159-d27a-41d5-b71e-ca1ecff6542b\" # PPE SQL Server Connection\n          PROD: \"6c517159-d27a-41d5-b71e-ca1ecff6542b\" # PROD SQL Server Connection\n      item_type: \"DataPipeline\" # filter on data pipeline files\n\n    # Schedule enabled state to be replaced\n    - find_key: $.schedules[?(@.jobType==\"Execute\")].enabled\n      replace_value:\n          PPE: false # disable execution in PPE environment\n          PROD: true # enable execution in PROD environment\n      file_path: \"**/.schedules\" # filter on all .schedules files\n\nspark_pool:\n    - instance_pool_id: \"72c68dbc-0775-4d59-909d-a47896f4573b\" # spark_pool_instance_id to be replaced\n      replace_value:\n          PPE:\n              type: \"Capacity\" # target spark pool type, only supports Capacity or Workspace\n              name: \"CapacityPool_Medium\" # target spark pool name\n          PROD:\n              type: \"Capacity\" # target spark pool type, only supports Capacity or Workspace\n              name: \"CapacityPool_Large\" # target spark pool name\n      item_name: \"World\" # filter on environment file for environment named \"World\"\n\n    - instance_pool_id: \"e7b8f1c4-4a6e-4b8b-9b2e-8f1e5d6a9c3d\" # spark_pool_instance_id to be replaced\n      replace_value:\n          PPE:\n              type: \"Workspace\" # target spark pool type, only supports Capacity or Workspace\n              name: \"WorkspacePool_Medium\" # target spark pool name\n      item_name: [\"World_1\", \"World_2\", \"World_3\"] # filter on environment files for environments with these names\n</code></pre>"},{"location":"how_to/parameterization/#examples-by-item-type","title":"Examples by Item Type","text":""},{"location":"how_to/parameterization/#notebooks","title":"Notebooks","text":""},{"location":"how_to/parameterization/#find_replace-parameterization-case","title":"<code>find_replace</code> Parameterization Case","text":"<p>Case: A Notebook is attached to a Lakehouse which resides in different workspaces. The Workspace and Lakehouse GUIDs in the Notebook need to be updated to ensure the Notebook points to the correct Lakehouse once deployed.</p> <p>Solution: In the <code>notebook-content.py</code> file, the default_lakehouse <code>47592d55-9a83-41a8-9b21-e1ef44264161</code>, and default_lakehouse_workspace_id <code>2190baad-a374-4114-addd-0dcf0533e69d</code> must be replaced with the corresponding GUIDs of the Lakehouse in the target environment (PPE/PROD/etc). This replacement is managed by the <code>find_replace</code> input in the <code>parameter.yml</code> file where fabric-cicd finds every instance of the string within the specified repository files and replaces it with the string for the deployed environment.</p> <p>parameter.yml file</p> <pre><code>find_replace:\n    - find_value: \"47592d55-9a83-41a8-9b21-e1ef44264161\" # lakehouse GUID to be replaced\n      replace_value:\n          PPE: \"a21e502a-51a5-4455-bb3d-6faf1e3e21fb\" # PPE lakehouse GUID\n          PROD: \"1069f2ff-bb30-42a0-97b3-1f4655705b8a\" # PROD lakehouse GUID\n      item_type: \"Notebook\" # filter on notebook files\n      item_name: [\"Hello World\", \"Goodbye World\"] # filter on specific notebook files\n    - find_value: \"2190baad-a374-4114-addd-0dcf0533e69d\" # workspace ID to be replaced\n      replace_value:\n          PPE: \"5a6ebbe6-9289-4105-b47c-cf158247b911\" # PPE workspace ID\n          PROD: \"f9e8cbe0-2669-4e06-a026-7c75e5af8107\" # PROD workspace ID\n      file_path: # filter on notebook files with these paths\n          - \"/Hello World.Notebook/notebook-content.py\"\n          - \"\\\\Goodbye World.Notebook\\\\notebook-content.py\"\n</code></pre> <p>notebook-content.py file</p> <pre><code># Fabric notebook source\n\n# METADATA ********************\n\n# META {\n# META   \"kernel_info\": {\n# META     \"name\": \"synapse_pyspark\"\n# META   },\n# META   \"dependencies\": {\n# META     \"lakehouse\": {\n# META       \"default_lakehouse\": \"47592d55-9a83-41a8-9b21-e1ef44264161\",\n# META       \"default_lakehouse_name\": \"Example_LH\",\n# META       \"default_lakehouse_workspace_id\": \"2190baad-a374-4114-addd-0dcf0533e69d\"\n# META     },\n# META     \"environment\": {\n# META       \"environmentId\": \"a277ea4a-e87f-8537-4ce0-39db11d4aade\",\n# META       \"workspaceId\": \"00000000-0000-0000-0000-000000000000\"\n# META     }\n# META   }\n# META }\n\n# CELL ********************\n\ndf = spark.sql(\"SELECT * FROM Example_LH.Table1 LIMIT 1000\")\ndisplay(df)\n\n# METADATA ********************\n\n# META {\n# META   \"language\": \"python\",\n# META   \"language_group\": \"synapse_pyspark\"\n# META }\n</code></pre>"},{"location":"how_to/parameterization/#advanced-find_replace-parameterization-case","title":"Advanced <code>find_replace</code> Parameterization Case","text":"<p>Case: A Notebook is attached to a Lakehouse which resides in the same workspace. When deploying both the Lakehouse and the Notebook to a target environment (PPE/PROD/etc), the Workspace and Lakehouse GUIDs referenced in the Notebook must be updated to ensure it correctly points to the corresponding Lakehouse in the new environment.</p> <p>Solution: This approach uses <code>find_value</code> regex** and dynamic variables to manage replacement. In the <code>find_replace</code> input in the <code>parameter.yml</code> file, the <code>is_regex</code> field is set to <code>\"true\"</code>, enabling fabric-cicd to find a string value within the specified repository files that matches the provided regex pattern.</p> <p>This approach is particularly useful for replacing values that are not known until deployment time, such as item IDs.</p> <p>**The regex pattern must include a capture group, defined using <code>()</code>, and the <code>find_value</code> must always match group 1. The value captured in this group will be dynamically replaced with the appropriate value for the deployed environment.</p> <p>parameter.yml file</p> <pre><code>find_replace:\n    # lakehouse GUID matching group 1 of regex pattern to be replaced\n    - find_value: \\#\\s*META\\s+\"default_lakehouse\":\\s*\"([0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12})\"\n      replace_value:\n          PPE: \"$items.Lakehouse.Example_LH.id\" # PPE lakehouse GUID (dynamic)\n          PROD: \"$items.Lakehouse.Example_LH.id\" # PROD lakehouse GUID (dynamic)\n      is_regex: \"true\"\n      item_type: \"Notebook\" # filter on notebook files\n      item_name: [\"Hello World\", \"Goodbye World\"] # filter on specific notebook files\n    # workspace ID matching group 1 of regex pattern to be replaced\n    - find_value: \\#\\s*META\\s+\"default_lakehouse_workspace_id\":\\s*\"([0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12})\"\n      replace_value:\n          PPE: \"$workspace.id\" # PPE workspace ID (dynamic)\n          PROD: \"$workspace.id\" # PROD workspace ID (dynamic)\n      is_regex: \"true\"\n      file_path: # filter on notebook files with these paths\n          - \"/Hello World.Notebook/notebook-content.py\"\n          - \"\\\\Goodbye World.Notebook\\\\notebook-content.py\"\n</code></pre> <p>notebook-content.py file</p> <pre><code># Fabric notebook source\n\n# METADATA ********************\n\n# META {\n# META   \"kernel_info\": {\n# META     \"name\": \"synapse_pyspark\"\n# META   },\n# META   \"dependencies\": {\n# META     \"lakehouse\": {\n# META       \"default_lakehouse\": \"123e4567-e89b-12d3-a456-426614174000\",\n# META       \"default_lakehouse_name\": \"Example_LH\",\n# META       \"default_lakehouse_workspace_id\": \"8f5c0cec-a8ea-48cd-9da4-871dc2642f4c\"\n# META     },\n# META     \"environment\": {\n# META       \"environmentId\": \"a277ea4a-e87f-8537-4ce0-39db11d4aade\",\n# META       \"workspaceId\": \"00000000-0000-0000-0000-000000000000\"\n# META     }\n# META   }\n# META }\n\n# CELL ********************\n\ndf = spark.sql(\"SELECT * FROM Example_LH.Table1 LIMIT 1000\")\ndisplay(df)\n\n# METADATA ********************\n\n# META {\n# META   \"language\": \"python\",\n# META   \"language_group\": \"synapse_pyspark\"\n# META }\n</code></pre>"},{"location":"how_to/parameterization/#data-pipelines","title":"Data Pipelines","text":""},{"location":"how_to/parameterization/#key_value_replace-parameterization-case","title":"<code>key_value_replace</code> Parameterization Case","text":"<p>Case: A Data Pipeline is attached to data sources via the Connection Id. Connections are not deployed with fabric-cicd and therefore need to be parameterized. In the <code>pipeline-content.json</code> file, the SQL Server Connection Id <code>c517e095-ed87-4665-95fa-8cdb1e751fba</code>, must be replaced with the corresponding GUIDs of the SQL Server in the target environment (PPE/PROD/etc).</p> <p>Solution: This replacement is managed by the <code>find_key</code> input in the <code>parameter.yml</code> file where fabric-cicd finds every instance of the key within the specified repository files and replaces it with the string for the deployed environment.</p> <p>parameter.yml file</p> <pre><code>key_value_replace:\n    - find_key: $.properties.activities[?(@.name==\"Copy Data\")].typeProperties.source.datasetSettings.externalReferences.connection\n      replace_value:\n          PPE: \"f47ac10b-58cc-4372-a567-0e02b2c3d479\" # PPE SQL Connection GUID\n          PROD: \"9b2e5f4c-8d3a-4f1b-9c3e-2d5b6e4a7f8c\" # PROD SQL Connection GUID\n      item_type: \"DataPipeline\" # filter on Data Pipelines files\n      item_name: \"Example Pipeline\" # filter on specific Data Pipelines files\n</code></pre> <p>pipeline-content.json file</p> <pre><code>{\n    \"properties\": {\n        \"activities\": [\n            {\n                \"name\": \"Copy Data\",\n                \"type\": \"Copy\",\n                \"dependsOn\": [],\n                \"policy\": {\n                    \"timeout\": \"0.12:00:00\",\n                    \"retry\": 0,\n                    \"retryIntervalInSeconds\": 30,\n                    \"secureOutput\": false,\n                    \"secureInput\": false\n                },\n                \"typeProperties\": {\n                    \"source\": {\n                        \"type\": \"AzureSqlSource\",\n                        \"queryTimeout\": \"02:00:00\",\n                        \"partitionOption\": \"None\",\n                        \"datasetSettings\": {\n                            \"annotations\": [],\n                            \"type\": \"AzureSqlTable\",\n                            \"schema\": [],\n                            \"typeProperties\": {\n                                \"schema\": \"Dataprod\",\n                                \"table\": \"DIM_Calendar\",\n                                \"database\": \"unified\"\n                            },\n                            \"externalReferences\": {\n                                \"connection\": \"c517e095-ed87-4665-95fa-8cdb1e751fba\"\n                            }\n                        }\n                    },\n                    \"sink\": {\n                        \"type\": \"LakehouseTableSink\",\n                        \"tableActionOption\": \"Append\",\n                        \"datasetSettings\": {\n                            \"annotations\": [],\n                            \"linkedService\": {\n                                \"name\": \"Unified\",\n                                \"properties\": {\n                                    \"annotations\": [],\n                                    \"type\": \"Lakehouse\",\n                                    \"typeProperties\": {\n                                        \"workspaceId\": \"2d2e0ae2-9505-4f0c-ab42-e76cc11fb07d\",\n                                        \"artifactId\": \"31dd665e-95f3-4575-9f46-70ea5903d89b\",\n                                        \"rootFolder\": \"Tables\"\n                                    }\n                                }\n                            },\n                            \"type\": \"LakehouseTable\",\n                            \"schema\": [],\n                            \"typeProperties\": {\n                                \"schema\": \"Dataprod\",\n                                \"table\": \"DIM_Calendar\"\n                            }\n                        }\n                    },\n                    \"enableStaging\": false,\n                    \"translator\": {\n                        \"type\": \"TabularTranslator\",\n                        \"typeConversion\": true,\n                        \"typeConversionSettings\": {\n                            \"allowDataTruncation\": true,\n                            \"treatBooleanAsNumber\": false\n                        }\n                    }\n                }\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"how_to/parameterization/#schedules","title":"Schedules","text":""},{"location":"how_to/parameterization/#key_value_replace-parameterization-case_1","title":"<code>key_value_replace</code> Parameterization Case","text":"<p>Case: Items with schedules need to have their execution settings parameterized across environments. The <code>enabled</code> field in <code>.schedules</code> files typically needs different values for different environments (e.g., disabled in test environments, enabled in production).</p> <p>Solution: In the <code>.schedules</code> file, the <code>enabled</code> field for Execute jobs must be replaced with environment-specific boolean values. This replacement is managed by the <code>key_value_replace</code> input in the <code>parameter.yml</code> file where fabric-cicd finds the JSONPath expression within the specified repository files and replaces it with the appropriate value for the deployed environment.</p> <p>parameter.yml file</p> <pre><code>key_value_replace:\n    - find_key: $.schedules[?(@.jobType==\"Execute\")].enabled\n      replace_value:\n          PPE: false\n          PROD: true\n      file_path: \"**/.schedules\"\n</code></pre> <p>.schedules file</p> <pre><code>{\n    \"$schema\": \"https://developer.microsoft.com/json-schemas/fabric/gitIntegration/schedules/1.0.0/schema.json\",\n    \"schedules\": [\n        {\n            \"enabled\": true,\n            \"jobType\": \"Execute\",\n            \"configuration\": {\n                \"type\": \"Cron\",\n                \"startDateTime\": \"2025-07-01T12:00:00\",\n                \"endDateTime\": \"2029-07-01T12:00:00\",\n                \"localTimeZoneId\": \"Pacific Standard Time\",\n                \"interval\": 15\n            }\n        }\n    ]\n}\n</code></pre> <p>Note: The <code>.schedules</code> file contains JSON content but does not have a <code>.json</code> file extension. The fabric-cicd library automatically detects and processes files with valid JSON content regardless of their file extension, making this parameterization work seamlessly.</p>"},{"location":"how_to/parameterization/#environments","title":"Environments","text":""},{"location":"how_to/parameterization/#spark_pool-parameterization-case","title":"<code>spark_pool</code> Parameterization Case","text":"<p>Case: An Environment is attached to a Capacity level Custom Pool. Source control for Environments does not output the right fields necessary to deploy, so the Spark Pool needs to be parameterized. Note: Defining different names per environment is supported in the <code>parameter.yml</code> file. In the <code>Sparkcompute.yaml</code> file, the referenced instance_pool_id <code>72c68dbc-0775-4d59-909d-a47896f4573b</code> points to a capacity custom pool named <code>CapacityPool_Large</code> of pool type <code>Capacity</code> for the <code>PROD</code> environment.</p> <p>Solution: This replacement is managed by the <code>spark_pool</code> input in the <code>parameter.yml</code> file where fabric-cicd finds every instance of the <code>instance_pool_id</code> and replaces it with the pool type and pool name for the specified environment file.</p> <p>parameter.yml file</p> <pre><code>spark_pool:\n    - instance_pool_id: \"72c68dbc-0775-4d59-909d-a47896f4573b\" # spark_pool_instance_id to be replaced\n      replace_value:\n          PPE:\n              type: \"Capacity\" # target spark pool type, only supports Capacity or Workspace\n              name: \"CapacityPool_Medium\" # target spark pool name\n          PROD:\n              type: \"Capacity\" # target spark pool type, only supports Capacity or Workspace\n              name: \"CapacityPool_Large\" # target spark pool name\n      item_name: \"World\" # filter on environment file for environment named \"World\"\n</code></pre> <p>Sparkcompute.yml</p> <pre><code>enable_native_execution_engine: false\ninstance_pool_id: 72c68dbc-0775-4d59-909d-a47896f4573b\ndriver_cores: 16\ndriver_memory: 112g\nexecutor_cores: 16\nexecutor_memory: 112g\ndynamic_executor_allocation:\n    enabled: false\n    min_executors: 31\n    max_executors: 31\nruntime_version: 1.3\n</code></pre>"},{"location":"how_to/parameterization/#dataflows","title":"Dataflows","text":"<p>Dataflows can have different kinds of Fabric sources and destinations that need to be parameterized, depending on the scenario.</p>"},{"location":"how_to/parameterization/#parameterization-overview","title":"Parameterization Overview","text":"<p>Take a Lakehouse source/destination as an example, the Lakehouse is connected to a Dataflow in the following ways:</p> <ol> <li>Connection Id in the <code>queryMetadata.json</code> file:<ul> <li>Connections are not deployed with fabric-cicd and therefore need to be parameterized.</li> </ul> </li> <li>Workspace and item IDs in the <code>mashup.pq</code> file:<ul> <li>Source and/or destination item references, such as a Dataflow (source only**), Lakehouse, Warehouse, etc. appear in the <code>mashup.pq</code> file and need to be parameterized to ensure proper deployment across environments.</li> </ul> </li> </ol> <p>**Note: A Dataflow that sources from another Dataflow introduces a dependency that may require a specific order of deploying (source first then dependent). A Dataflow is referenced by the item ID in the workspace and the actual workspace ID, this makes re-pointing more complex (see parameterization guidance below).</p>"},{"location":"how_to/parameterization/#parameterization-guidance","title":"Parameterization Guidance","text":"<p>Connections must be parameterized in addition to item references.</p> <p>Scenarios When Deploying a Dataflow that contains a source Dataflow reference:</p> <ol> <li> <p>Source Dataflow exists in the same workspace as the dependent Dataflow:</p> <ul> <li>The source Dataflow must be deployed BEFORE the dependent Dataflow (especially during first time deployment).</li> <li>To handle this dependency correctly and prevent deployment errors, set up the <code>find_replace</code> parameter with the following requirements (incorrect setup may introduce failure during Dataflow deployment):<ul> <li>Set <code>find_value</code> to match the <code>dataflowId</code> GUID referenced in the <code>mashup.pq</code> file (literal string or regex).</li> <li>Set <code>replace_value</code> to the variable <code>$items.Dataflow.&lt;The Source Dataflow Name&gt;.id</code>. Important: Make sure the item type is <code>\"Dataflow\"</code> and the item name matches the source Dataflow name in the repository directory exactly (case sensitive, include any spaces).</li> <li>File filters are optional but recommended when using a regex pattern for <code>find_value</code>.</li> <li>You don't need to parameterize the source Dataflow workspace ID here as the library automatically handles this replacement when you use the items variable in this Dataflow scenario.</li> </ul> </li> <li>How this works: This parameterization approach ensures correct deployment of interdependent Dataflows while automatically updating references to point to the newly deployed Dataflow in the target workspace.</li> <li>Example parameter input:</li> </ul> <pre><code>find_replace:\n    # The ID of the source Dataflow referenced in mashup.pq\n    - find_value: \"0187104d-7a35-4abe-a2ca-a241ec81c8f1\"\n      # Type = Dataflow and Name = &lt;The Source Dataflow Name&gt;, Attribute = id\n      replace_value:\n          PPE: \"$items.Dataflow.Source Dataflow.id\"\n          PROD: \"$items.Dataflow.Source Dataflow.id\"\n      # Optional fields:\n      file_path:\n          - \"\\\\Referencing Dataflow.Dataflow\\\\mashup.pq\"\n</code></pre> </li> <li> <p>Source Dataflow exists in a different workspace from the dependent Dataflow:</p> <ul> <li>When the source Dataflow exists in a different workspace, deployment order doesn't matter.</li> <li>To re-point the source Dataflow from one workspace to another workspace, you can parameterize using the <code>find_replace</code> parameter. The Dataflow ID AND Workspace ID of the source Dataflow both need to be parameterized.</li> <li>Note: dynamic replacement for item ID and workspace ID will NOT work here since the source Dataflow does not exist in the repository directory.</li> </ul> </li> </ol> <p>Scenarios When Deploying a Dataflow that contains other Fabric items (e.g., Lakehouse, Warehouse, etc.) references:</p> <ol> <li> <p>Source and/or destination item exists in the same workspace as the dependent Dataflow:</p> <ul> <li>Use the <code>find_replace</code> parameter to update references so they point to the corresponding items in the target workspace.</li> <li>You need to parameterize both the item ID and workspace ID found in the <code>mashup.pq</code> file.</li> <li>Best practices for Dataflow parameterization:<ul> <li>Use a regex for the <code>find_value</code> to avoid hardcoding GUIDs and simplify maintenance</li> <li>Use dynamic replacement to eliminate multi-phase deployments</li> </ul> </li> <li>Adding file filters to target specific Dataflow files provides more precise control.</li> </ul> </li> <li> <p>Source/destination item exists in a different workspace from the dependent Dataflow:</p> <ul> <li>Use the <code>find_replace</code> parameter to update references so they point to items in the different workspace.</li> <li>Parameterize both the item ID and workspace ID found in the <code>mashup.pq</code> file.</li> <li>Use a regex pattern for the <code>find_value</code> to avoid hardcoding GUIDs and simplify maintenance.</li> <li>Note: dynamic replacement won't work in this scenario - it only works for items in the same workspace as the Dataflow.</li> <li>Adding file filters helps target specific Dataflow files for more precise control.</li> </ul> </li> </ol>"},{"location":"how_to/parameterization/#advanced-find_replace-parameterization-case_1","title":"Advanced <code>find_replace</code> Parameterization Case","text":"<p>Case: A Dataflow points to a destination Lakehouse. The Lakehouse exists in the same workspace as the Dataflow. In the <code>mashup.pq</code> file, the following GUIDs need to be replaced:</p> <ul> <li>The workspaceId <code>e6a8c59f-4b27-48d1-ae03-7f92b1c6458d</code> with the target workspace Id.</li> <li>The lakehouseId <code>3d72f90e-61b5-42a8-9c7e-b085d4e31fa2</code> with the corresponding Id of the Lakehouse in the target environment (PPE/PROD/etc).</li> </ul> <p>Solution: These replacements are managed using a regex pattern as input for the <code>find_value</code> in the <code>parameter.yml</code> file, which finds the matching value in the specified repository files and replaces it with the dynamically retrieved workspace or item Id of the target environment.</p> <p>Note: While Connection IDs are shown in this example, they are not the main focus. Connection parameterization may vary depending on your specific scenario.</p> <p>parameter.yml file</p> <pre><code>find_replace:\n    # Lakehouse workspace ID regex - matches the workspaceId GUID\n    - find_value: Navigation_1\\s*=\\s*Pattern\\{\\[workspaceId\\s*=\\s*\"([0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12})\"\\]\\}\n      replace_value:\n          PPE: \"$workspace.id\" # PPE workspace ID (dynamic)\n          PROD: \"$workspace.id\"\n      is_regex: \"true\" # Activate find_value regex matching\n      file_path: \"/Sample Dataflow.Dataflow/mashup.pq\"\n\n    # Lakehouse ID regex - matches the lakehouseId GUID\n    - find_value: Navigation_2\\s*=\\s*Navigation_1\\{\\[lakehouseId\\s*=\\s*\"([0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12})\"\\]\\}\n      replace_value:\n          PPE: \"$items.Lakehouse.Sample_LH.id\" # Sample_LH Lakehouse ID in PPE (dynamic)\n          PROD: \"$items.Lakehouse.Sample_LH.id\"\n      is_regex: \"true\" # Activate find_value regex matching\n      file_path: \"/Sample Dataflow.Dataflow/mashup.pq\"\n\n    # Connection ID - Cluster ID\n    - find_value: \"8e4f92a7-3c18-49d5-b6d0-7f2e591ca4e8\"\n      replace_value:\n          PPE: \"76a8f5c3-e4b2-48d1-9c7f-382d69a5e7b0\" # PPE Cluster ID\n          PROD: \"f297e14d-6c83-42a5-b718-59d40e3f8c2d\" # PROD Cluster ID\n      file_path: \"/Sample Dataflow.Dataflow/mashup.pq\"\n\n    # Connection ID - Datasource ID\n    - find_value: \"d12c5f7b-90a3-47e6-8d2c-3fb59e01d47a\"\n      replace_value:\n          PPE: \"25b9a417-3d8e-4f62-901c-75de6ba84f35\" # PPE Datasource ID\n          PROD: \"cb718d96-5ae2-47fc-8b93-1d24c0f5e8a7\" # PROD Datasource ID\n      file_path: \"/Sample Dataflow.Dataflow/mashup.pq\"\n</code></pre> <p>queryMetadata.json file</p> <pre><code>{\n    \"formatVersion\": \"202502\",\n    \"computeEngineSettings\": {},\n    \"name\": \"Sample Dataflow\",\n    \"queryGroups\": [],\n    \"documentLocale\": \"en-US\",\n    \"queriesMetadata\": {\n        \"Table\": {\n            \"queryId\": \"ba67667b-14c0-4536-a92d-feafc73baa4b\",\n            \"queryName\": \"Table\",\n            \"loadEnabled\": false\n        },\n        \"Table_DataDestination\": {\n            \"queryId\": \"a157a378-b510-4d95-bb82-5a7c80df8b4c\",\n            \"queryName\": \"Table_DataDestination\",\n            \"isHidden\": true,\n            \"loadEnabled\": false\n        }\n    },\n    \"connections\": [\n        {\n            \"path\": \"Lakehouse\",\n            \"kind\": \"Lakehouse\",\n            \"connectionId\": \"{\\\"ClusterId\\\":\\\"8e4f92a7-3c18-49d5-b6d0-7f2e591ca4e8\\\",\\\"DatasourceId\\\":\\\"d12c5f7b-90a3-47e6-8d2c-3fb59e01d47a\\\"}\"\n        }\n    ]\n}\n</code></pre> <p>mashup.pq file</p> <pre><code>[StagingDefinition = [Kind = \"FastCopy\"]]\nsection Section1;\n[DataDestinations = {[Definition = [Kind = \"Reference\", QueryName = \"Table_DataDestination\", IsNewTarget = true], Settings = [Kind = \"Automatic\", TypeSettings = [Kind = \"Table\"]]]}]\nshared Table = let\n  Source = Table.FromRows(Json.Document(Binary.Decompress(Binary.FromText(\"i45WckksSUzLyS9X0lEyBGKP1JycfKVYHYhEQGZBak5mXipQwgiIw/OLclLAkn75JalJ+fnZQEFjmC4FhHRwam5iXklmsm9+SmoOUN4EiMFsBVTzoRabArGLG0x/LAA=\", BinaryEncoding.Base64), Compression.Deflate)), let _t = ((type nullable text) meta [Serialized.Text = true]) in type table [Item = _t, Id = _t, Name = _t]),\n  #\"Changed column type\" = Table.TransformColumnTypes(Source, {{\"Item\", type text}, {\"Id\", Int64.Type}, {\"Name\", type text}}),\n  #\"Added custom\" = Table.TransformColumnTypes(Table.AddColumn(#\"Changed column type\", \"IsDataflow\", each if [Item] = \"Dataflow\" then true else false), {{\"IsDataflow\", type logical}}),\n  #\"Added custom 1\" = Table.TransformColumnTypes(Table.AddColumn(#\"Added custom\", \"ContainsHello\", each if Text.Contains([Name], \"Hello\") then 1 else 0), {{\"ContainsHello\", Int64.Type}})\nin\n  #\"Added custom 1\";\nshared Table_DataDestination = let\n  Pattern = Lakehouse.Contents([CreateNavigationProperties = false, EnableFolding = false]),\n  Navigation_1 = Pattern{[workspaceId = \"e6a8c59f-4b27-48d1-ae03-7f92b1c6458d\"]}[Data],\n  Navigation_2 = Navigation_1{[lakehouseId = \"3d72f90e-61b5-42a8-9c7e-b085d4e31fa2\"]}[Data],\n  TableNavigation = Navigation_2{[Id = \"Items\", ItemKind = \"Table\"]}?[Data]?\nin\n  TableNavigation;\n</code></pre>"}]}